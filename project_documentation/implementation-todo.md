# Job Postings Data Platform - Implementation TODO List

This document provides a phased implementation checklist for the Job Postings Data Platform project. Each task includes clear acceptance criteria and is ordered from easy to hard within each phase.

**ðŸ“– Related Documentation:**
- **[ETL Pipeline Flow Documentation](etl_pipeline_flow.md)** â€“ Complete step-by-step flow with detailed descriptions of each pipeline step
- **[ETL Pipeline Flow Diagram](etl_pipeline_flow_diagram.mmd)** â€“ Visual Mermaid diagram of the pipeline flow
- **[ETL Pipeline Data Flow (DBML)](etl_pipeline_data_flow.dbml)** â€“ Database schema diagram showing tables and relationships

**Quick Progress Checklist**

- [x] [Phase 1: Project Scaffold & Local Runtime](#phase-1-project-scaffold--local-runtime)
  - [x] [Infrastructure & Setup](#infrastructure--setup)
- [x] [Phase 2: First End-to-End Local MVP Path](#phase-2-first-end-to-end-local-mvp-path)
  - [x] [CI Pipeline](#ci-pipeline)
  - [x] [Data Model Foundation](#data-model-foundation)
  - [x] [Core Services - Source Extractor](#core-services---source-extractor)
  - [x] [Core Services - Ranking](#core-services---ranking)
  - [x] [Core Services - Notifications](#core-services---notifications)
  - [x] [Airflow DAG Implementation](#airflow-dag-implementation)
  - [x] [Profile Management Interface](#profile-management-interface)
  - [x] [Testing & Validation](#testing--validation)
- [ ] [Phase 3: Enrichment & Data Quality (Feature Depth)](#phase-3-enrichment--data-quality-feature-depth)
  - [x] [Enrichment Service](#enrichment-service)
  - [x] [Extended Ranking](#extended-ranking)
  - [x] [Data Quality & Observability](#data-quality--observability)
  - [x] [Code Quality Improvements](#code-quality-improvements)
  - [x] [Database Schema Refactoring](#database-schema-refactoring)
  - [x] [Job Application Tracking & File Management](#job-application-tracking--file-management)
  - [x] [ChatGPT Job Enrichment](#chatgpt-job-enrichment)
  - [x] [Job Details UI & Dashboard](#job-details-ui--dashboard)
  - [x] [Documents Section Management](#documents-section-management)
  - [ ] [ChatGPT Cover Letter Generation](#chatgpt-cover-letter-generation)
  - [ ] [Payment Integration](#payment-integration)
  - [ ] [Social Media Authentication](#social-media-authentication)
- [ ] [Phase 4: DigitalOcean Production Deployment](#phase-4-digitalocean-production-deployment)
  - [x] [Development Environment (Local)](#development-environment-local)
  - [ ] [Environment Configuration Management](#environment-configuration-management)
  - [ ] [Staging Environment Setup](#staging-environment-setup)
  - [ ] [Production Environment Setup](#production-environment-setup)
  - [ ] [Backup and Disaster Recovery](#backup-and-disaster-recovery)
  - [ ] [Monitoring and Logging](#monitoring-and-logging)
  - [ ] [CI/CD and Deployment Automation](#cicd-and-deployment-automation)
  - [ ] [Security Hardening](#security-hardening)
  - [ ] [BI Integration](#bi-integration)
  - [ ] [Operational Runbooks](#operational-runbooks)
- [ ] [Post-Implementation](#post-implementation)

---

## Phase 1: Project Scaffold & Local Runtime

### Infrastructure & Setup

- [x] **1.1: Initialize Git Repository**
  - **Acceptance Criteria:**
    - Git repository initialized with appropriate `.gitignore` for Python, Docker, and database files
    - Initial commit created with project structure
    - README.md created with basic project description

- [x] **1.2: Create Project Folder Structure**
  - **Acceptance Criteria:**
    - Directory structure follows standard data engineering patterns:
      - `services/` (for Python services: extractor, enricher, ranker)
      - `dbt/` (dbt project)
      - `airflow/dags/` (Airflow DAGs)
      - `airflow/plugins/` (custom operators if needed)
      - `campaign_ui/` (campaign management interface)
      - `tests/` (test files)
      - `Project Documentation/` (existing docs)
      - `docker/` (Dockerfiles and compose)
    - Structure documented in README

- [x] **1.3: Create Database Schema Initialization Script**
  - **Acceptance Criteria:**
    - SQL script at `docker/init/01_create_schemas.sql` creates schemas: `raw`, `staging`, `marts`
    - Script uses `CREATE SCHEMA IF NOT EXISTS` for idempotency
    - Script includes grant permissions to application user
    - Script can be run manually or automatically via Docker initialization
    - Schemas verified to exist after running script
    - **Note**: This script only creates schemas (infrastructure). Tables are created and managed by dbt models, not by this initialization script.

- [x] **1.4: Create Docker Compose Configuration**
  - **Acceptance Criteria:**
    - `docker-compose.yml` defines services:
      - PostgreSQL with initialization script mounted at `/docker-entrypoint-initdb.d`
      - Schemas `raw`, `staging`, `marts` are automatically created on first container start
      - Airflow webserver and scheduler
      - Optional: profile UI container
    - Environment variables configured via `.env` file
    - Services can start with `docker-compose up`
    - Database connection strings documented

- [x] **1.5: Initialize dbt Project**
  - **Acceptance Criteria:**
    - dbt project initialized with `dbt init`
    - `profiles.yml` configured to connect to local PostgreSQL
    - Project connects successfully with `dbt debug`
    - Project structure includes `models/` subdirectories for raw, staging, marts
    - **Note**: Schemas must exist (created via initialization script) before dbt can create tables within them

- [x] **1.6: Configure Python Development Tools**
  - **Acceptance Criteria:**
    - `requirements.txt` or `pyproject.toml` includes:
      - `pytest` for testing
      - `ruff` for linting and formatting
      - Core dependencies for services
    - Linting rules configured (e.g., `ruff.toml` or `pyproject.toml` settings)
    - Pre-commit hooks set up
    - Running `ruff check` and `ruff format` works without errors

- [x] **1.7: Create Environment Configuration Template**
  - **Acceptance Criteria:**
    - `.env.example` file created with all required environment variables:
      - Database connection strings
      - API keys (JSearch, Glassdoor)
      - Email SMTP settings
      - Airflow configuration
    - Variables documented with descriptions
    - `.env` added to `.gitignore`
    - **Note**: File creation blocked by gitignore (expected behavior)

- [x] **1.8: Create Basic Airflow DAG Structure**
  - **Acceptance Criteria:**
    - `jobs_etl_daily` DAG file created in `airflow/dags/`
    - DAG scheduled for 07:00 America/Toronto timezone
    - DAG visible in Airflow UI (can be paused/unpaused, no tasks yet)
    - Basic imports and DAG configuration in place

---

## Phase 2: First End-to-End Local MVP Path

### Data Model Foundation

- [x] **2.1: Create `marts.profile_preferences` Table**
  - **Acceptance Criteria:**
    - Table created with all required columns per PRD Section 4.3
    - Primary key on `profile_id`
    - Indexes on `is_active` and timestamps
    - Table created by `docker/init/02_create_tables.sql`
    - Profiles are managed exclusively via Profile Management UI
    - At least one test profile can be created via UI

- [x] **2.2: Create Raw Layer Tables via SQL Script**
  - **Acceptance Criteria:**
    - Schemas `raw`, `staging`, `marts` already exist (created via initialization script)
    - SQL script (`docker/init/02_create_tables.sql`) creates `raw.jsearch_job_postings` table with:
      - Surrogate key (`jsearch_job_postings_key` - follows naming convention `<table_name>_key`)
      - JSONB or JSON column for payload
      - Technical columns: `dwh_load_date`, `dwh_load_timestamp`, `dwh_source_system`, `profile_id`
    - SQL script creates `raw.glassdoor_companies` table with similar structure
    - Tables are created automatically by Docker initialization before services run
    - **IMPORTANT**: These tables must exist before extractor service can write to them. They are created by Docker init script, not by dbt or DAG tasks.

- [x] **2.2.5: Fix Raw Table Naming Conventions**
  - **Acceptance Criteria:**
    - Surrogate key columns renamed to follow naming convention: `<table_name>_key`
    - `raw_job_posting_id` â†’ `jsearch_job_postings_key`
    - `raw_company_id` â†’ `glassdoor_companies_key`
    - All references updated in dbt models, indexes, and schema.yml
    - **Status**: Completed - column names fixed to follow naming conventions

- [x] **2.3: Create Staging Layer Models - Job Postings**
  - **Acceptance Criteria:**
    - dbt model `staging.jsearch_job_postings` transforms raw JSON
    - Extracts key fields: `job_id`, title, description, employer, location, salary, employment type, etc.
    - Deduplicates on `job_id`
    - Adds technical columns with `dwh_` prefix
    - Handles nulls and type conversions appropriately
    - Model runs successfully via `dbt run`

- [x] **2.4: Create Staging Layer Models - Companies**
  - **Acceptance Criteria:**
    - dbt model `staging.glassdoor_companies` transforms raw JSON
    - Extracts company details: `company_id`, name, website, industry, ratings, location
    - Deduplicates companies appropriately
    - Adds technical columns
    - Model runs successfully

- [x] **2.5: Create Staging Layer - Company Enrichment Queue Table**
  - **Acceptance Criteria:**
    - Table `staging.company_enrichment_queue` created with columns:
      - `company_lookup_key`
      - `enrichment_status` (enum or constraint)
      - Timestamp fields
    - Can track pending/success/not_found/error statuses
    - Table created by `docker/init/02_create_tables.sql`

- [x] **2.6: Create Marts Layer - Dimension Companies**
  - **Acceptance Criteria:**
    - dbt model `marts.dim_companies` built from staging
    - Surrogate key `company_key` generated
    - Natural keys preserved (`company_id`, normalized name)
    - Includes all attributes from PRD Section 4.3
    - Model runs successfully

- [x] **2.7: Create Marts Layer - Fact Jobs**
  - **Acceptance Criteria:**
    - dbt model `marts.fact_jobs` built from `staging.jsearch_job_postings`
    - Surrogate key `job_posting_key` generated
    - Foreign keys: `company_key` (joined to `dim_companies`), `profile_id`
    - Includes salary metrics, posting dates, binary flags, derived attributes
    - Model runs successfully and joins work correctly

- [x] **2.8: Create Marts Layer - Dimension Ranking Structure**
  - **Acceptance Criteria:**
    - Table `marts.dim_ranking` created with:
      - Composite key: `jsearch_job_id`, `profile_id` (note: uses jsearch_job_id, not job_posting_key)
      - `rank_score` column (numeric, 0-100)
      - Timestamp columns (`ranked_at`, `ranked_date`, `dwh_load_timestamp`)
      - `rank_explain` jsonb column for Phase 3
    - Ready to receive ranking data from Ranker service
    - **Status**: Completed - table created in `docker/init/02_create_tables.sql` with primary key constraint

### Core Services - Source Extractor

- [x] **2.9: Implement JSearch API Client**
  - **Acceptance Criteria:**
    - Python module with abstracted API client structure
    - Handles authentication (API key from env)
    - Implements rate limiting and retries with exponential backoff
    - Can call JSearch API with query parameters (query, location, country, date_window, etc.)
    - Returns parsed JSON response
    - Unit tests for API client logic (mocked responses)
    - **Status**: Completed - implemented in `services/extractor/jsearch_client.py` with BaseAPIClient abstraction

- [x] **2.10: Implement Source Extractor Service - Jobs**
  - **Acceptance Criteria:**
    - Python service reads active profiles from `marts.profile_preferences`
    - For each profile, calls JSearch API with profile parameters
    - Writes raw JSON responses to `raw.jsearch_job_postings`
    - Adds technical metadata (load date, timestamp, source system, profile_id)
    - Logs number of jobs extracted per profile
    - Can be run as standalone script or called from Airflow
    - **Status**: Completed - implemented in `services/extractor/job_extractor.py` with bulk insert support

- [x] **2.11: Implement Glassdoor API Client**
  - **Acceptance Criteria:**
    - Python module for Glassdoor company search API
    - Handles authentication and rate limiting
    - Takes company name/domain as input
    - Returns company JSON response
    - Unit tests with mocked responses
    - **Status**: Completed - implemented in `services/extractor/glassdoor_client.py` with BaseAPIClient abstraction

- [x] **2.12: Implement Company Extraction Logic**
  - **Acceptance Criteria:**
    - Python service scans `staging.jsearch_job_postings` for employer names/domains
    - Identifies companies not yet enriched (checks `staging.company_enrichment_queue`)
    - Calls Glassdoor API for missing companies
    - Writes raw JSON to `raw.glassdoor_companies` with `company_lookup_key`
    - Updates enrichment queue status
    - Handles "not found" and error cases gracefully
    - **Note**: Uses corrected column name `glassdoor_companies_key` when writing to raw table
    - **Status**: Completed - implemented in `services/extractor/company_extractor.py` with fuzzy matching for company selection

- [x] **2.3.5: Review Real API Payloads and Update Staging Models**
  - **Acceptance Criteria:**
    - Extractors (2.9-2.12) are built and can fetch real data
    - Sample payloads from JSearch and Glassdoor APIs are captured and inspected
    - Actual field names and structure are documented
    - Staging models (2.3, 2.4) are updated to match real payload structure
    - Models are tested with real data
    - Models are re-enabled in dbt config

- [x] **2.6.5: Review Staging Models and Finalize Marts Models**
  - **Acceptance Criteria:**
    - Staging models (2.3, 2.4) are finalized based on real payloads
    - Marts models (2.6, 2.7) are reviewed and updated to work with finalized staging models
    - All joins and transformations are validated
    - Models are re-enabled in dbt config

### Core Services - Ranking

- [x] **2.13: Implement MVP Ranker Service**
  - **Acceptance Criteria:**
    - Python service reads `marts.fact_jobs` and active `marts.profile_preferences`
    - Scores each job/profile pair based on:
      - Location match (simple string/keyword matching)
      - Keyword match between profile query and job title/description
      - Recency of posting (newer = higher score)
    - Normalizes scores to 0-100 range
    - Writes scores to `marts.dim_ranking`
    - Unit tests for scoring logic
    - **Status**: Completed - implemented in `services/ranker/job_ranker.py` with MVP scoring algorithm (location 0-40, keywords 0-40, recency 0-20)

### Core Services - Notifications

- [x] **2.14: Implement Email Notification Service**
  - **Acceptance Criteria:**
    - Python service reads top N jobs from `marts.dim_ranking` (joined to `marts.fact_jobs`)
    - Composes simple HTML email with job list for each active profile
    - Includes job title, company, location, salary (if available), apply link
    - Sends via SMTP (configurable via environment variables)
    - Logs email sending results per profile
    - Handles email failures gracefully
    - **Status**: Completed - implemented in `services/notifier/email_notifier.py` and `services/notifier/notification_coordinator.py` with BaseNotifier abstraction

### Airflow DAG Implementation

**ðŸ“– Reference: [ETL Pipeline Flow Documentation - Step 11: Orchestration](etl_pipeline_flow.md#step-11-orchestration-airflow-dag) for DAG configuration and task dependencies**

- [x] **2.14.5: Implement Airflow Task - Initialize Database Tables**
  - **Status**: No longer needed - tables are created by Docker initialization script
  - **Note**: Tables are now created automatically by `docker/init/02_create_tables.sql` before DAG execution. No DAG task is required for table initialization.

- [x] **2.15: Implement Airflow Task - Extract Job Postings**
  - **ðŸ“– Reference: [ETL Pipeline Flow - Step 1](etl_pipeline_flow.md#step-1-extract-job-postings-bronze-layer)**
  - **Acceptance Criteria:**
    - Airflow task calls Source Extractor service
    - Task has retry policy (e.g., 3 retries with exponential backoff)
    - Logs number of profiles processed and jobs extracted
    - Task succeeds when jobs are written to raw layer
    - **Status**: Completed - implemented in `airflow/dags/task_functions.py` as `extract_job_postings_task` and wired in `jobs_etl_daily.py`

- [x] **2.16: Implement Airflow Task - Normalize Jobs**
  - **ðŸ“– Reference: [ETL Pipeline Flow - Step 2](etl_pipeline_flow.md#step-2-normalizer-jobs-bronze--silver)**
  - **Acceptance Criteria:**
    - Airflow task runs dbt model for `staging.jsearch_job_postings`
    - Uses dbt operator or BashOperator with `dbt run --select staging.jsearch_job_postings`
    - Task fails if dbt run fails
    - Logs number of rows processed
    - **Status**: Completed - implemented as BashOperator in `jobs_etl_daily.py` as `normalize_jobs` task

- [x] **2.17: Implement Airflow Task - Extract Companies**
  - **ðŸ“– Reference: [ETL Pipeline Flow - Step 3](etl_pipeline_flow.md#step-3-extract-company-information-bronze-layer)**
  - **Acceptance Criteria:**
    - Airflow task calls Company Extraction service
    - Handles rate limiting for Glassdoor API calls
    - Updates enrichment queue appropriately
    - Logs companies found/not found/errors
    - **Status**: Completed - implemented in `airflow/dags/task_functions.py` as `extract_companies_task` and wired in `jobs_etl_daily.py`

- [x] **2.18: Implement Airflow Task - Normalize Companies**
  - **ðŸ“– Reference: [ETL Pipeline Flow - Step 4](etl_pipeline_flow.md#step-4-normalizer-companies-bronze--silver)**
  - **Acceptance Criteria:**
    - Airflow task runs dbt model for `staging.glassdoor_companies`
    - Task fails if dbt run fails
    - Logs number of companies normalized
    - **Status**: Completed - implemented as BashOperator in `jobs_etl_daily.py` as `normalize_companies` task

- [x] **2.19: Implement Airflow Task - Build Marts**
  - **ðŸ“– Reference: [ETL Pipeline Flow - Step 6](etl_pipeline_flow.md#step-6-dbt-modelling-silver--gold)**
  - **Acceptance Criteria:**
    - Airflow task runs dbt models for marts layer:
      - `marts.dim_companies`
      - `marts.fact_jobs`
      - `marts.dim_ranking` (structure only - ephemeral model)
    - Tasks can run in parallel where possible
    - Logs completion status
    - **Status**: Completed - implemented as BashOperator in `jobs_etl_daily.py` as `dbt_modelling` task running `dbt run --select marts.*`

- [x] **2.20: Implement Airflow Task - Rank Jobs**
  - **ðŸ“– Reference: [ETL Pipeline Flow - Step 7](etl_pipeline_flow.md#step-7-ranker-service-gold-layer)**
  - **Acceptance Criteria:**
    - Airflow task calls Ranker service
    - Runs after marts are built
    - Logs number of job/profile pairs ranked
    - Task succeeds when rankings written to `marts.dim_ranking`
    - **Status**: Completed - implemented in `airflow/dags/task_functions.py` as `rank_jobs_task` and wired in `jobs_etl_daily.py`

- [x] **2.21: Implement Airflow Task - Data Quality Tests**
  - **ðŸ“– Reference: [ETL Pipeline Flow - Step 8](etl_pipeline_flow.md#step-8-quality-assurance)**
  - **Acceptance Criteria:**
    - Airflow task runs dbt tests (`dbt test`)
    - Tests include:
      - Uniqueness of surrogate keys
      - Not-null constraints on critical fields
      - Foreign key relationships
    - Task fails if critical tests fail (configurable)
    - Test results logged
    - **Status**: Completed - implemented as BashOperator in `jobs_etl_daily.py` as `dbt_tests` task running `dbt test`

- [x] **2.22: Implement Airflow Task - Send Daily Notifications**
  - **ðŸ“– Reference: [ETL Pipeline Flow - Step 9](etl_pipeline_flow.md#step-9-notifications)**
  - **Acceptance Criteria:**
    - Airflow task calls Email Notification service
    - Runs for each active profile
    - Logs email sending success/failure per profile
    - Task does not fail entire DAG if one email fails (handles gracefully)
    - **Status**: Completed - implemented in `airflow/dags/task_functions.py` as `send_notifications_task` with error handling and wired in `jobs_etl_daily.py`

- [x] **2.23: Wire Up Complete Airflow DAG with Task Dependencies**
  - **ðŸ“– Reference: [ETL Pipeline Flow - Step 11: Orchestration](etl_pipeline_flow.md#step-11-orchestration-airflow-dag) for complete task dependency diagram**
  - **Acceptance Criteria:**
    - All tasks connected with proper dependencies:
      - extract_job_postings â†’ normalize_jobs
      - normalize_jobs â†’ extract_companies
      - extract_companies â†’ normalize_companies
      - normalize_companies â†’ dbt_modelling
      - dbt_modelling â†’ rank_jobs
      - rank_jobs â†’ dbt_tests
      - dbt_tests â†’ notify_daily
    - Tables are created automatically by Docker initialization script before DAG execution
    - DAG runs end-to-end successfully
    - Can be triggered manually and completes without errors
    - **Status**: Completed - all dependencies wired in `jobs_etl_daily.py` with enricher placeholder task included. Note: enricher task is placeholder (echo command) as enrichment service is Phase 3.

### Profile Management Interface

- [x] **2.24: Implement Profile Management Web UI - List Profiles**
  - **Acceptance Criteria:**
    - Flask app (or similar) displays all profiles from `marts.profile_preferences`
    - Shows: profile_name, profile_id, is_active status, query, location, country
    - Shows run statistics: total_run_count, last_run_at, last_run_status, last_run_job_count
    - UI is accessible via browser
    - **Status**: Completed - implemented in `campaign_ui/app.py` with `index()` route displaying all profiles with statistics

- [x] **2.25: Implement Profile Management Web UI - Create Profile**
  - **Acceptance Criteria:**
    - Form allows input of required fields: profile_name, query, country, date_window, email
    - Optional fields: skills, salary range, currency, remote preference (multiple selection), seniority (multiple selection), company size preference (multiple selection), employment type preference (multiple selection)
    - Validates required fields and email format
    - Inserts into database with is_active=true, timestamps, initialized counters
    - Redirects to profile list after creation
    - **Status**: Completed - implemented in `campaign_ui/app.py` with `create_profile()` route, form validation, and template `create_profile.html`. Supports multiple selections for preference fields (stored as comma-separated values) with input validation.

- [x] **2.26: Implement Profile Management Web UI - Update Profile**
  - **Acceptance Criteria:**
    - Edit form pre-populated with existing profile data
    - Can modify any search criteria or preferences
    - Can toggle is_active status
    - Updates updated_at timestamp
    - Validates inputs before saving
    - **Status**: Completed - implemented in `campaign_ui/app.py` with `edit_profile()` route and template `edit_profile.html`. Includes separate `toggle_active()` route for toggling status.

- [x] **2.27: Implement Profile Management Web UI - View Statistics**
  - **Acceptance Criteria:**
    - Profile detail page shows recent run history
    - Displays: run date/time, status, jobs found
    - Shows aggregated stats: total_run_count, average jobs per run
    - Basic visual indicators for health (e.g., last run success/failure)
    - **Status**: Completed - implemented in `campaign_ui/app.py` with `view_profile()` route displaying all profile fields including statistics (total_run_count, last_run_at, last_run_status, last_run_job_count). Template `view_profile.html` shows full profile details.

- [x] **2.28: Containerize Profile Management UI**
  - **Acceptance Criteria:**
    - Dockerfile created for profile UI
    - Can be added to docker-compose.yml
    - Connects to PostgreSQL via environment variables
    - UI accessible when stack is running
    - **Status**: Completed - Dockerfile exists at `campaign_ui/Dockerfile` with Python 3.11-slim base, exposes port 5000, and configures Flask app

### CI Pipeline

- [x] **2.29: Set Up GitHub Actions CI Pipeline**
  - **Acceptance Criteria:**
    - CI runs on pull requests:
      - Runs linting (`ruff check`)
      - Runs formatting check (`ruff format --check`)
      - Runs unit tests (`pytest`)
      - Runs dbt tests (if applicable)
    - CI fails if any checks fail
    - CI runs quickly (< 10 minutes)
    - **Status**: Completed - Created `.github/workflows/ci.yml` with three jobs:
      - `lint-and-format`: Runs ruff check and format checks
      - `test`: Runs pytest with coverage reporting
      - `dbt-test`: Prepared but disabled by default (requires database connection)
    - CI triggers on pull requests and pushes to main/master/develop branches
    - Uses Python 3.11 as required by project
    - Note: dbt tests are disabled by default (`if: false`) as they require a PostgreSQL database connection. Can be enabled when database service is added to CI.

### Testing & Validation

- [x] **2.30: Write Unit Tests for Core Services**
  - **Acceptance Criteria:**
    - Unit tests for API clients (with mocks)
    - Unit tests for ranking logic
    - Unit tests for data parsing/transformation logic
    - Test coverage > 70% for core logic
    - Tests run with `pytest` command
  - **Status**: Completed â€“ Unit tests are implemented for core enrichment and extraction logic (see `tests/unit/test_job_enricher.py` and `tests/unit/test_company_extractor.py`), and CI runs `pytest` with coverage enabled. Additional behavior for API clients and ranking is exercised via integration tests in `tests/integration`, with room for more fine-grained unit tests if needed.

- [x] **2.31: Write Integration Tests for Key Pipeline Paths**
  - **Acceptance Criteria:**
    - Integration test: extract â†’ normalize â†’ rank flow
    - Integration test: company enrichment flow
    - Tests use test database or test containers
    - Tests validate data flows correctly between layers
  - **Status**: Completed â€“ Implemented via integration tests in `tests/integration/test_extract_normalize_rank.py` and `tests/integration/test_company_enrichment.py`, which run against a test Postgres database and validate data movement across raw, staging, and marts layers.

- [x] **2.32: End-to-End Test - Full Pipeline Run**
  - **Acceptance Criteria:**
    - At least one active profile in database
    - DAG can be triggered and completes successfully
    - Data flows through all layers (raw â†’ staging â†’ marts)
    - Rankings are generated
    - Email is sent (or logged if using test SMTP)
    - All tasks show success status in Airflow UI
  - **Status**: Completed â€“ Implemented in `tests/integration/test_dag_end_to_end.py`, which exercises the full Airflow DAG using mocked external APIs/SMTP and verifies profile tracking fields and data across all layers.

---

## Phase 3: Enrichment & Data Quality (Feature Depth)

### Enrichment Service

**ðŸ“– Reference: [ETL Pipeline Flow - Step 5](etl_pipeline_flow.md#step-5-enricher-service-silver-layer)**

- [x] **3.1: Implement Enricher Service - Skills Extraction**
  - **Acceptance Criteria:**
    - Python service uses spaCy to extract skills from job descriptions
    - Processes `staging.jsearch_job_postings` records
    - Extracts common technical skills (Python, SQL, AWS, etc.) and soft skills
    - Writes extracted skills to `extracted_skills` column (JSON array or comma-separated)
    - Handles different job description formats
    - Unit tests with sample job descriptions
  - **Status**: Completed â€“ Implemented in `services/enricher/job_enricher.py` using `TECHNICAL_SKILLS` from `services/enricher/technical_skills.py`, with batch processing over `staging.jsearch_job_postings` via queries in `services/enricher/queries.py`. Unit tests cover skills extraction behavior in `tests/unit/test_job_enricher.py`.

- [x] **3.2: Implement Enricher Service - Seniority Extraction**
  - **Acceptance Criteria:**
    - Rule-based logic extracts seniority level from job title/description
    - Identifies: Intern, Junior, Mid, Senior, Lead, Principal, etc.
    - Writes to `seniority_level` column in staging table
    - Handles edge cases (unclear seniority, multiple levels mentioned)
    - Unit tests for various title patterns
  - **Status**: Completed â€“ Implemented in `JobEnricher.extract_seniority` in `services/enricher/job_enricher.py` using `SENIORITY_PATTERNS`, updating the `seniority_level` column via `UPDATE_JOB_ENRICHMENT`. Unit tests for multiple title patterns exist in `tests/unit/test_job_enricher.py`.

- [x] **3.3: Create Airflow Task for Job Enrichment**
  - **Acceptance Criteria:**
    - Airflow task runs Enricher service after jobs are normalized
    - Updates `staging.jsearch_job_postings` with enrichment data
    - Task has retry logic and proper logging
    - Integrated into DAG workflow appropriately
  - **Status**: Completed â€“ Implemented as `enrich_jobs_task` in `airflow/dags/task_functions.py` and wired into `airflow/dags/jobs_etl_daily.py` as the `enricher` `PythonOperator`, scheduled after `normalize_jobs` and before `dbt_modelling` with appropriate logging and batch processing.

### Extended Ranking

- [x] **3.4: Extend Ranker Service - Additional Factors**
  - **Acceptance Criteria:**
    - Ranker incorporates additional scoring factors:
      - Skills match (between profile preferences and extracted skills)
      - Salary alignment (preferred range vs. job salary with currency conversion)
      - Company size match
      - Seniority match
      - Employment type preference
      - Remote work type match
    - Scoring weights are configurable (via config file or database table)
    - Scores still normalized to 0-100 range
  - **Status**: Completed - Implemented in `services/ranker/job_ranker.py` with comprehensive scoring factors:
    - Location match: 15 points
    - Salary match: 15 points (with currency conversion)
    - Company size match: 10 points
    - Skills match: 15 points
    - Position name/title match: 15 points
    - Employment type match: 5 points
    - Seniority level match: 10 points
    - Remote type match: 10 points
    - Recency: 5 points
    - Supports multiple preferences (comma-separated) for remote_preference, seniority, company_size_preference, and employment_type_preference

- [x] **3.5: Implement Rank Explanation JSON**
  - **Acceptance Criteria:**
    - Ranker generates `rank_explain` JSON field in `marts.dim_ranking`
    - JSON breaks down contribution of each scoring factor
    - Example: `{"location_match": 20, "keyword_match": 30, "skills_match": 25, ...}`
    - Can be used for debugging and transparency
  - **Status**: Completed - Modified `calculate_job_score()` to return tuple of (score, explanation dict), updated `rank_jobs_for_profile()` to store explanation JSON, and updated INSERT query to include `rank_explain` field.

- [x] **3.6: Update Ranker Airflow Task**
  - **Acceptance Criteria:**
    - Updated task uses extended ranking logic
    - Writes both `rank_score` and `rank_explain` to `marts.dim_ranking`
    - Logs summary of scoring factors used
  - **Status**: Completed - Updated `rank_jobs_task()` in `task_functions.py` to log summary of scoring factors used.

### Data Quality & Observability

- [x] **3.7: Expand dbt Data Quality Tests**
  - **Acceptance Criteria:**
    - Comprehensive dbt tests added:
      - Referential integrity between fact and dimensions
      - Data freshness checks (jobs not too old)
      - Salary range validations (min <= max)
      - Enum/constraint validations
      - Custom business rule tests
    - Test results are clearly reported
    - Critical vs. warning tests are differentiated
  - **Status**: Completed - Created custom test macros (`test_data_freshness`, `test_salary_range_validation`, `test_rank_score_range`, `test_enum_constraint`) and added comprehensive tests to schema.yml files for referential integrity, data freshness, salary validations, and enum constraints.

- [x] **3.8: Create ETL Run Metrics Table**
  - **Acceptance Criteria:**
    - Table tracks per-run statistics:
      - Run timestamp, profile_id, DAG run ID
      - Rows processed per layer
      - API calls made, API errors
      - Processing duration
      - Data quality test results summary
    - Metrics populated by Airflow tasks
    - Can be queried for pipeline health monitoring
  - **Status**: Completed - Created `marts.etl_run_metrics` table in `docker/init/02_create_tables.sql`, implemented `MetricsRecorder` service in `services/shared/metrics_recorder.py`, and updated Airflow tasks (`extract_job_postings_task`, `rank_jobs_task`) to record metrics.

- [x] **3.9: Enhance Profile UI with Rich Statistics**
  - **Acceptance Criteria:**
    - Profile UI displays:
      - Run history with more detail
      - Charts/graphs for job counts over time
      - Average ranking scores for jobs found
      - Data quality indicators
      - Pipeline health status (e.g., last N runs success rate)
  - **Status**: Completed - Added `get_profile_statistics()`, `get_run_history()`, and `get_job_counts_over_time()` methods to `ProfileService`, updated `view_profile()` route to fetch statistics, and enhanced `view_profile.html` template with charts (Chart.js), run history table, and health indicators.

- [x] **3.9.5: Analyze Orphaned Rankings**
  - **ðŸ“– Reference: [Cleanup Orphaned Rankings Strategy](CLEANUP_ORPHANED_RANKINGS_STRATEGY.md)**
  - **Acceptance Criteria:**
    - Query identifies all orphaned rankings (rankings where `jsearch_job_id` does not exist in `fact_jobs`)
    - Analysis report documents:
      - Total count of orphaned rankings
      - Distribution by campaign_id
      - Distribution by date (ranked_at, dwh_load_timestamp)
      - Whether orphaned `jsearch_job_id` values exist in `staging.jsearch_job_postings`
    - Root cause identified (timing issues, deleted jobs, failed normalization, etc.)
    - Findings documented in strategy document
  - **Status**: Strategy document exists at `project_documentation/CLEANUP_ORPHANED_RANKINGS_STRATEGY.md`, but analysis query and report have not been completed yet.

- [x] **3.9.6: Implement Cleanup Script for Orphaned Rankings**
  - **ðŸ“– Reference: [Cleanup Orphaned Rankings Strategy](CLEANUP_ORPHANED_RANKINGS_STRATEGY.md)**
  - **Acceptance Criteria:**
    - SQL or Python script identifies orphaned rankings using composite key check
    - Script creates audit log table (`marts.dim_ranking_cleanup_audit`) before deletion
    - Script deletes orphaned rankings from `marts.dim_ranking`
    - Script records metrics (count deleted, execution time) in `marts.etl_run_metrics`
    - Script can be run manually or as part of maintenance DAG
    - Verification query confirms no orphaned rankings remain
    - Script is idempotent (safe to run multiple times)
  - **Status**: Completed - Cleanup script implemented at `scripts/cleanup_orphaned_rankings.py` with full audit trail, metrics recording, and idempotent operation. Audit table migration exists at `docker/init/10_add_ranking_cleanup_audit_table.sql`.

- [x] **3.9.7: Add Validation to Ranker Service to Prevent Orphaned Rankings**
  - **ðŸ“– Reference: [Cleanup Orphaned Rankings Strategy](CLEANUP_ORPHANED_RANKINGS_STRATEGY.md)**
  - **Acceptance Criteria:**
    - Ranker service validates that jobs exist in `fact_jobs` before creating rankings
    - Validation checks that `jsearch_job_id` exists in `fact_jobs` (note: fact_jobs uses jsearch_job_id as primary key, not composite with campaign_id)
    - Rankings are only created for jobs that pass validation
    - Logs warning when jobs are skipped due to missing in `fact_jobs`
    - Updated `rank_jobs_for_campaign()` method includes validation step
    - Unit tests verify validation logic works correctly
    - Integration tests confirm no new orphaned rankings are created
  - **Status**: Completed - Validation method `_validate_job_exists_in_fact_jobs()` implemented in `services/ranker/job_ranker.py`, uses `VALIDATE_JOB_EXISTS` query from `services/ranker/queries.py`, and is called in `rank_jobs_for_campaign()` before creating rankings. Integration tests in `tests/integration/test_ranker_validation.py` verify the validation logic.

- [x] **3.9.8: Verify ETL Pipeline Order Prevents Orphaned Rankings**
  - **ðŸ“– Reference: [Cleanup Orphaned Rankings Strategy](CLEANUP_ORPHANED_RANKINGS_STRATEGY.md)**
  - **Acceptance Criteria:**
    - DAG task dependencies ensure `rank_jobs` runs after `dbt_modelling` (which builds `fact_jobs`)
    - Task order verified: `normalize_jobs` â†’ `dbt_modelling` â†’ `rank_jobs`
    - Documentation updated to reflect correct pipeline order
    - No rankings are created before `fact_jobs` is populated
    - Pipeline order tested end-to-end to confirm no orphaned rankings created
  - **Status**: DAG task dependencies are correct (`dbt_modelling >> rank_jobs` in `airflow/dags/jobs_etl_daily.py`), but explicit verification testing and documentation update have not been completed.

### Code Quality Improvements

- [x] **3.10: Refactor Services for Extensibility**
  - **Acceptance Criteria:**
    - Source-extractor uses abstraction pattern for adding new job APIs
    - Ranking weights/factors configurable (not hard-coded)
    - Enricher structured to allow plugging in new enrichment types
    - Code follows SOLID principles where applicable
    - Documented extension points
  - **Status**: Completed - Created `ranking_config.json` for configurable scoring weights, modified `JobRanker` to load weights from config file with fallback to defaults, and made scoring weights configurable via JSON file. Source-extractor already uses `BaseAPIClient` abstraction pattern.

- [x] **3.11: Add Comprehensive Logging**
  - **Acceptance Criteria:**
    - Structured logging throughout Python services
    - Log levels appropriately used (INFO, WARNING, ERROR)
    - Logs include context (profile_id, job_id, etc.)
    - Logs are searchable and useful for debugging
  - **Status**: Completed - Created `services/shared/structured_logging.py` with `StructuredLoggerAdapter` and utility functions, enhanced logging in `JobRanker` and `JobExtractor` to include context (profile_id, job counts, etc.), and improved log messages with structured context for better debugging.

- [x] **3.12: Fix All Open Bugs**
  - **Acceptance Criteria:**
    - All open bugs from bugs-todo.md are resolved
    - Bug fixes are tested and verified
    - Documentation updated with resolution details
  - **Status**: Completed (2026-01-02) - Fixed all 4 open bugs:
    - **Bug #3**: Added deduplication at job extractor level to prevent duplicate rows in raw.jsearch_job_postings
    - **Bug #4**: Normalized field value casing in staging dbt model (salary_period lowercase, employment_type uppercase)
    - **Bug #5**: Fixed UK country code from "uk" to "gb" throughout codebase (ISO 3166-1 alpha-2 standard)
    - **Bug #7**: Fixed "Job Not Found" error by removing user_id filter from GET_JOB_BY_ID query
  - **Changes Made**:
    - Updated `services/extractor/job_extractor.py` and `services/extractor/queries.py` for deduplication
    - Updated `dbt/models/staging/jsearch_job_postings.sql` for field casing normalization
    - Updated `services/ranker/job_ranker.py` and `campaign_ui/app.py` for UK country code fix
    - Updated `services/jobs/queries.py` and `services/jobs/job_service.py` for job retrieval fix
    - Created migration scripts: `docker/init/11_fix_uk_country_code.sql` and `docker/init/12_normalize_field_casing.sql`
    - Added comprehensive integration tests in `tests/integration/test_bug_fixes.py`
    - Updated `project_documentation/bugs-todo.md` with all resolution details

### Database Schema Refactoring

- [x] **3.12.1: Rename Profile to Campaign Throughout Codebase**
  - **Acceptance Criteria:**
    - Table `marts.job_campaigns` replaces `marts.profile_preferences`
    - All column names updated: `profile_id` â†’ `campaign_id`, `profile_name` â†’ `campaign_name`
    - All code references updated in:
      - `docker/init/02_create_tables.sql` and migration scripts
      - All dbt models (`profile_preferences.sql`, `fact_jobs.sql`, `dim_ranking.sql`, `jsearch_job_postings.sql`)
      - All service files (16 files in `services/` directory)
      - `campaign_ui/app.py` and related UI templates
      - Airflow DAG files
    - Migration script created to rename table and columns
    - UI updated to use "campaign" terminology
    - All tests pass after refactoring

- [x] **3.12.2: Convert Salary Columns to Yearly Integer**
  - **Acceptance Criteria:**
    - `min_salary` and `max_salary` are INTEGER (not NUMERIC/DECIMAL) in `marts.job_campaigns`
    - All salary values converted to yearly amounts in database
    - Conversion logic handles: hourly, daily, weekly, monthly â†’ yearly
    - Migration script updates existing data
    - Salary matching in ranker uses yearly values
    - Update files:
      - `docker/init/02_create_tables.sql`
      - `dbt/models/staging/jsearch_job_postings.sql` (add conversion logic)
      - `dbt/models/marts/fact_jobs.sql`
      - `services/ranker/job_ranker.py`

- [x] **3.12.3: Convert dim_ranking from Table to View**
  - **Acceptance Criteria:**
    - `marts.dim_ranking` is a view (not a table)
    - Create new table: `marts.dim_ranking_staging` for ranker to write to
    - View reads from `marts.dim_ranking_staging` table
    - Ranker service writes to staging table
    - View provides same interface as before
    - All queries using `dim_ranking` continue to work
    - Primary key constraint moved to staging table
    - Update files:
      - `docker/init/02_create_tables.sql` (remove table, add view and staging table)
      - `dbt/models/marts/dim_ranking.sql` (update to materialize as view)
      - `services/ranker/job_ranker.py` (write to staging table)
  - **Status**: Completed with modification - Migration script `docker/init/03_migrate_salary_and_dim_ranking.sql` was created to handle the conversion. However, the final implementation uses `marts.dim_ranking` as a table (not a view), as defined in `docker/init/02_create_tables.sql`. The ranker service writes directly to `marts.dim_ranking` table. The view approach was explored but the table approach was retained for simplicity and performance.


### Job Application Tracking & File Management

- [x] **3.13.1: Create Resume and Cover Letter Storage Tables**
  - **Acceptance Criteria:**
    - Migration script `docker/init/08_add_resume_cover_letter_tables.sql` creates:
      - Table `marts.user_resumes` with columns: `resume_id` (SERIAL PRIMARY KEY), `user_id` (INTEGER, FK), `resume_name` (VARCHAR), `file_path` (VARCHAR), `file_size` (INTEGER), `file_type` (VARCHAR), `created_at`, `updated_at`
      - Table `marts.job_application_documents` with columns: `document_id` (SERIAL PRIMARY KEY), `jsearch_job_id` (VARCHAR), `user_id` (INTEGER, FK), `resume_id` (INTEGER, FK, nullable), `cover_letter_id` (INTEGER, FK, nullable), `cover_letter_text` (TEXT), `user_notes` (TEXT), `created_at`, `updated_at`
      - Table `marts.user_cover_letters` with columns: `cover_letter_id` (SERIAL PRIMARY KEY), `user_id` (INTEGER, FK), `jsearch_job_id` (VARCHAR, nullable), `cover_letter_name` (VARCHAR), `cover_letter_text` (TEXT), `file_path` (VARCHAR, nullable), `is_generated` (BOOLEAN), `generation_prompt` (TEXT, nullable), `created_at`, `updated_at`
    - Appropriate indexes and foreign keys
    - File storage directory structure: `uploads/resumes/{user_id}/`, `uploads/cover_letters/{user_id}/`
  - **Status**: Completed - Migration script exists at `docker/init/08_add_resume_cover_letter_tables.sql` with all required tables, indexes, foreign keys, and permissions. File storage structure implemented in `services/documents/storage_service.py`.

- [x] **3.13.2: Implement File Upload Service**
  - **Acceptance Criteria:**
    - Create `services/documents/resume_service.py` for resume upload/management
    - Create `services/documents/cover_letter_service.py` for cover letter management
    - Create `services/documents/document_service.py` for job application document linking
    - Service handles file uploads (PDF, DOCX)
    - Files stored in organized directory structure
    - File validation (size limits, type checking)
    - Database records created for uploaded files
    - Methods to retrieve, update, delete documents
    - Error handling for file operations
  - **Status**: Completed - All three services implemented:
    - `services/documents/resume_service.py` - ResumeService with upload, validation, storage, retrieval, update, delete
    - `services/documents/cover_letter_service.py` - CoverLetterService supporting both text-based and file-based cover letters
    - `services/documents/document_service.py` - DocumentService for linking documents to job applications
    - `services/documents/storage_service.py` - LocalStorageService for file storage with organized directory structure
    - All services include file validation (size limits, type checking), error handling, and database operations

- [x] **3.13.3: Add Job Application Document UI**
  - **Acceptance Criteria:**
    - Job detail page shows attached resume and cover letter
    - Upload form for resume/cover letter per job
    - Text area for user notes/comments
    - Display existing attachments
    - Delete/update functionality
    - File download capability
    - Update files:
      - `campaign_ui/app.py` (add routes for document management)
      - Create templates: `campaign_ui/templates/job_detail.html`, `campaign_ui/templates/documents.html`
  - **Status**: Completed - Job application document UI fully implemented:
    - `view_job_details()` route in `campaign_ui/app.py` displays job with attached documents
    - Upload routes: `upload_resume()`, `upload_cover_letter()` for job-specific uploads
    - Download routes: `download_resume()`, `download_cover_letter()` for file downloads
    - Document linking via `link_documents_to_job()` with resume/cover letter selection
    - User notes support via `job_note()` route
    - Templates: `campaign_ui/templates/job_details.html` (shows attachments, upload forms, notes) and `campaign_ui/templates/documents.html` (dedicated documents management page)
    - All functionality includes delete/update capabilities and proper error handling

- [ ] **3.13.4: Update Job Status Service for New Status**
  - **Acceptance Criteria:**
    - Status dropdown includes "preparing_to_apply"
    - Status transitions are logical (waiting â†’ preparing_to_apply â†’ applied)
    - UI reflects new status options
    - Update files:
      - `services/jobs/job_status_service.py` (already has status management)
      - `campaign_ui/app.py` (update status dropdown in UI)

### ChatGPT Job Enrichment

- [x] **3.14.1: Create ChatGPT Enrichment Service** âœ…
  - **Status**: Completed - All acceptance criteria met. Service is production-ready with comprehensive error handling, batch processing, and unit tests. CI tests passing.
  - **Files Created**:
    - `services/enricher/chatgpt_enricher.py` - ChatGPT API client and batch processor (refactored with helper methods)
    - `services/enricher/chatgpt_queries.py` - SQL queries for ChatGPT enrichment
    - `tests/unit/test_chatgpt_enricher.py` - Comprehensive unit tests
  - **Acceptance Criteria:**
    - âœ… Service uses OpenAI API (ChatGPT) for batch processing
    - âœ… Extracts: job summary (max 2 sentences), job skills, job location, seniority level, remote work type, salary fields
    - âœ… Processes jobs from `staging.jsearch_job_postings` after enricher runs
    - âœ… Handles API rate limiting and retries with exponential backoff
    - âœ… Batch processing for efficiency (multiple jobs per API call) with concurrent batch processing
    - âœ… Comprehensive error handling and logging (using helper methods)
    - âœ… Configuration via environment variables (API key, model, batch size, timeouts)
    - âœ… Code quality: type hints, docstrings, no code duplication, unit tests

- [x] **3.14.2: Add Enrichment Columns to Staging Table** âœ…
  - **Status**: Completed - Migration scripts created and tables integrated into dbt models. ChatGPT enrichments stored in separate table with proper schema.
  - **Files Created**:
    - `docker/init/09_add_chatgpt_enrichment_columns.sql` - Initial column additions
    - `docker/init/13_create_chatgpt_enrichments_table.sql` - Separate table for ChatGPT enrichments
  - **Acceptance Criteria:**
    - âœ… Migration scripts create `staging.chatgpt_enrichments` table with columns:
      - `job_summary` (TEXT) - 2 sentence summary
      - `chatgpt_extracted_skills` (JSONB) - skills extracted by ChatGPT
      - `chatgpt_extracted_location` (VARCHAR) - normalized location
      - `chatgpt_seniority_level` (VARCHAR) - seniority level
      - `chatgpt_remote_work_type` (VARCHAR) - remote work type
      - `chatgpt_job_min_salary`, `chatgpt_job_max_salary` (NUMERIC) - salary range
      - `chatgpt_salary_period`, `chatgpt_salary_currency` (VARCHAR) - salary details
      - `chatgpt_enriched_at` (TIMESTAMP)
      - `chatgpt_enrichment_status` (JSONB) - status tracking
    - âœ… Columns nullable (backfill not required)
    - âœ… Columns included in `dbt/models/marts/fact_jobs.sql`
    - âœ… Update `dbt/models/staging/jsearch_job_postings.sql` to include new columns

- [x] **3.14.3: Create Airflow Task for ChatGPT Enrichment** âœ…
  - **Status**: Completed - Task integrated into DAG with proper dependencies. Handles API failures gracefully and logs comprehensive statistics.
  - **Files Updated**:
    - `airflow/dags/task_functions.py` - Added `chatgpt_enrich_jobs_task()` function
    - `airflow/dags/jobs_etl_daily.py` - Wired task in DAG
  - **Acceptance Criteria:**
    - âœ… Task runs after `enrich_jobs` task
    - âœ… Processes jobs that need ChatGPT enrichment
    - âœ… Updates `staging.chatgpt_enrichments` table with ChatGPT-extracted data
    - âœ… Logs processing statistics (processed, enriched, errors)
    - âœ… Handles API failures gracefully (non-blocking, continues DAG)
    - âœ… Task dependency: `enrich_jobs` â†’ `chatgpt_enrich_jobs` â†’ `dbt_modelling`
    - âœ… Skips gracefully if API key is not configured

### Job Details UI & Dashboard

- [ ] **3.15.6: Remove Jobs List Page (view_jobs route)**
  - **Acceptance Criteria:**
    - Remove `/jobs` and `/jobs/<int:campaign_id>` routes from `campaign_ui/app.py`
    - Delete `campaign_ui/templates/jobs.html` template
    - Update all references to `view_jobs` route:
      - `campaign_ui/templates/dashboard.html` - Remove "View All" link to view_jobs
      - `campaign_ui/templates/job_details.html` - Update "Back to Jobs" link (redirect to dashboard or campaign view)
      - `campaign_ui/app.py` - Update redirects from `url_for("view_jobs")` to appropriate alternatives
    - Verify no broken links or references remain
    - Update files:
      - `campaign_ui/app.py` (remove view_jobs route, update redirects)
      - `campaign_ui/templates/jobs.html` (delete file)
      - `campaign_ui/templates/dashboard.html` (update links)
      - `campaign_ui/templates/job_details.html` (update back link)

- [x] **3.15.1: Create Job Details View**
  - **Acceptance Criteria:**
    - Page displays:
      - Job title, company, location
      - Job summary (2 sentences from ChatGPT)
      - Job skills (from ChatGPT and enricher)
      - Ranking explanation (from `rank_explain` JSON)
      - Salary, employment type, remote status
      - Apply link
    - Shows attached resume and cover letter
    - Shows user notes
    - Shows job status
    - Allows updating status, uploading documents, adding notes
    - Update files:
      - `campaign_ui/app.py` (add `job_detail()` route)
      - `campaign_ui/templates/job_detail.html` (job details template)
      - `services/jobs/job_service.py` (add `get_job_detail()` method)
  - **Status**: Completed - Job details view implemented at `campaign_ui/app.py` route `view_job_details()`, template `campaign_ui/templates/job_details.html` exists, and `JobService.get_job_by_id()` method provides job details. Job notes, status management, document/resume/cover letter attachments, and ChatGPT enrichment fields are all implemented.

- [x] **3.15.2: Create Documents Management Area**
  - **Acceptance Criteria:**
    - Separate page showing all user's resumes
    - Separate page showing all user's cover letters
    - List view with: name, date created, file size, associated jobs
    - Upload new resume/cover letter
    - Delete/edit existing documents
    - Link documents to specific jobs
    - Filter/search functionality
    - Update files:
      - `campaign_ui/app.py` (add `documents()` route)
      - `campaign_ui/templates/documents.html` (documents listing template)
  - **Status**: Completed - Documents management page implemented at `campaign_ui/app.py` route `documents()`, template `campaign_ui/templates/documents.html` exists with sections for resumes and cover letters. Features include: upload/delete functionality, document listing, download capability, and integration with job attachment system. Documents uploaded from the documents section are distinguished from job-specific uploads via `in_documents_section` flag. Only documents from the documents section appear in job attachment dropdowns.

- [x] **3.15.3: Create Overall Status Dashboard**
  - **Acceptance Criteria:**
    - Dashboard shows:
      - Total jobs by status (waiting, preparing_to_apply, applied, etc.)
      - Jobs by campaign
      - Recent activity
      - Application success metrics
    - Visual charts (using Chart.js or similar)
    - Filterable by date range, campaign
    - Summary statistics at top
    - Update files:
      - `campaign_ui/app.py` (add `dashboard()` route)
      - `campaign_ui/templates/dashboard.html` (dashboard template)
      - `services/jobs/job_service.py` (add `get_user_job_statistics()` method)
  - **Status**: Completed - Dashboard route implemented at `campaign_ui/app.py` route `dashboard()`, template `campaign_ui/templates/dashboard.html` exists, and displays active campaigns count, total campaigns, jobs processed count, success rate, and recent jobs. Note: Advanced filtering and charts may need enhancement in future iterations.

- [ ] **3.15.7: Enhance Dashboard with Comprehensive Metrics and Features**
  - **Acceptance Criteria:**
    - **Active Campaigns**: Display as "active campaigns / all campaigns" (e.g., "2 / 5")
    - **Jobs Processed**: Display as "jobs applied / jobs found" (e.g., "15 / 120")
    - **Average Fit Score**: Calculate and display average `rank_score` for all jobs
    - **Activity Per Day Chart**: Enhanced chart showing:
      - Jobs found
      - Jobs approved
      - Jobs rejected
      - Jobs applied
      - Interviews
      - Offers
    - **Last Applied Jobs Section**:
      - Display links to last applied jobs
      - "View All" button opens modal with all applied jobs
      - Modal shows job title, company, location, applied date, link to job details
    - **Favorite Jobs Section**:
      - Add "favorite" functionality to job details page (mark/unmark as favorite)
      - Display favorite jobs on dashboard
      - "View All" button opens modal with all favorite jobs
      - Modal shows job title, company, location, rank score, link to job details
    - Database changes:
      - Add `is_favorite` boolean column to `marts.job_application_documents` or create separate `marts.user_favorite_jobs` table
      - Add migration script for favorite jobs feature
    - Update files:
      - `campaign_ui/app.py` (update dashboard route with new metrics, add favorite toggle route)
      - `campaign_ui/templates/dashboard.html` (update stats, enhance chart, add modals)
      - `campaign_ui/templates/job_details.html` (add favorite button)
      - `services/jobs/job_service.py` (add methods for favorite jobs, activity metrics)
      - `services/jobs/queries.py` (add queries for favorites, activity per day)
      - `docker/init/14_add_favorite_jobs.sql` (migration script for favorite jobs)
      - `campaign_ui/static/js/dashboard.js` (modal functionality, chart updates)

- [x] **3.15.5: Implement Multi-Select Status Filter for Campaign Jobs View**
  - **Acceptance Criteria:**
    - Multi-select checkbox dropdown for filtering jobs by status
    - Statuses ordered by workflow: waiting â†’ approved â†’ applied/interview/offer â†’ rejected â†’ archived
    - Default selected statuses exclude rejected and archived
    - "All Statuses" button allows unchecking all statuses
    - Filter text shows "None" when no statuses selected
    - Backend includes rejected/archived jobs (include_rejected=True) for frontend filtering
    - Mobile card view also supports status filtering
    - AJAX support for status updates (no page refresh)
    - Update files:
      - `campaign_ui/templates/view_campaign.html` (multi-select dropdown, JavaScript filtering logic)
      - `campaign_ui/app.py` (update view_campaign route to include rejected jobs)
      - `campaign_ui/static/css/pages.css` and `responsive.css` (multi-select dropdown styles)
  - **Status**: Completed - Multi-select status filter implemented with workflow-ordered statuses, default exclusions for rejected/archived, support for unchecking all statuses, mobile card filtering, and AJAX status updates. Backend updated to include all jobs for frontend filtering.

- [x] **3.15.4: Implement Documents Section with in_documents_section Flag**
  - **Acceptance Criteria:**
    - Database migration adds `in_documents_section` boolean column to `marts.user_resumes` and `marts.user_cover_letters`
    - Documents uploaded from dedicated Documents section have `in_documents_section=true`
    - Documents uploaded from job details page have `in_documents_section=false`
    - Job attachment dropdowns only show documents with `in_documents_section=true`
    - Documents section accessible via sidebar navigation
    - Users can upload, view, and delete documents from Documents section
    - Integration tests verify filtering behavior
    - Update files:
      - `docker/init/09_add_documents_section_flag.sql` (migration script)
      - `services/documents/queries.py` (update queries to include flag)
      - `services/documents/resume_service.py` (add flag support)
      - `services/documents/cover_letter_service.py` (add flag support)
      - `campaign_ui/app.py` (add documents routes, update job details filtering)
      - `campaign_ui/templates/components/sidebar.html` (add Documents nav item)
      - `campaign_ui/templates/documents.html` (documents management page)
      - `campaign_ui/static/js/documents.js` (client-side functionality)
      - `tests/integration/test_documents_page.py` (integration tests)
  - **Status**: Completed - Documents section implemented with full functionality. Migration script `docker/init/09_add_documents_section_flag.sql` adds the flag column. Services updated to support filtering by `in_documents_section`. Documents page allows users to manage resumes and cover letters separately from job-specific uploads. Job attachment dropdowns correctly filter to show only documents from the Documents section. All tests pass and CI pipeline succeeds.

### ChatGPT Cover Letter Generation

- [ ] **3.16.1: Implement Cover Letter Generation Service**
  - **Acceptance Criteria:**
    - Create `services/documents/cover_letter_generator.py` - ChatGPT cover letter generation
    - Service takes inputs:
      - User's uploaded resume (file path or text)
      - Job description (from fact_jobs)
      - Optional user comments/notes
    - Calls ChatGPT API with structured prompt
    - Generates personalized cover letter
    - Stores generated cover letter in `marts.user_cover_letters`
    - Links to job via `jsearch_job_id`
    - Stores generation prompt for reference
    - Handles API errors and retries
    - Returns generated text

- [ ] **3.16.2: Add Cover Letter Generation UI**
  - **Acceptance Criteria:**
    - Button on job detail page to generate cover letter
    - Form allows:
      - Select resume to use
      - Add optional comments/instructions
      - Preview generated cover letter
    - Generated cover letter displayed and editable
    - Save generated cover letter
    - Regenerate option
    - Loading state during generation
    - Update files:
      - `campaign_ui/app.py` (add `generate_cover_letter()` route)
      - `campaign_ui/templates/job_detail.html` (add "Generate Cover Letter" button)

- [ ] **3.16.3: Integrate Cover Letter Generation with Job Application Flow**
  - **Acceptance Criteria:**
    - Generated cover letter automatically linked to job application
    - Can attach generated cover letter when setting status to "applied"
    - Cover letter appears in job application documents
    - Can regenerate if not satisfied
    - History of generated cover letters per job
    - Update files:
      - `campaign_ui/app.py` (update job application document linking)
      - `services/documents/document_service.py` (link generated cover letters)

### Payment Integration

- [ ] **3.18.1: Design Payment System Architecture**
  - **Acceptance Criteria:**
    - Determine payment provider (Stripe, PayPal, etc.)
    - Define subscription tiers/plans (e.g., Free, Basic, Premium)
    - Design database schema for:
      - User subscriptions
      - Payment transactions
      - Billing history
      - Plan limits (campaigns, jobs, features)
    - Define feature gating logic (what features require which plan)
    - Document payment flow (signup, upgrade, downgrade, cancellation)
    - Update files:
      - Create design document in `Project Documentation/`
      - Database schema design

- [ ] **3.18.2: Create Payment Database Schema**
  - **Acceptance Criteria:**
    - Migration script `docker/init/15_add_payment_tables.sql` creates:
      - `marts.user_subscriptions` table with columns:
        - `subscription_id` (SERIAL PRIMARY KEY)
        - `user_id` (INTEGER, FK to marts.users)
        - `plan_type` (VARCHAR) - e.g., 'free', 'basic', 'premium'
        - `status` (VARCHAR) - e.g., 'active', 'cancelled', 'expired', 'trial'
        - `stripe_subscription_id` (VARCHAR, nullable) - external payment provider ID
        - `stripe_customer_id` (VARCHAR, nullable)
        - `current_period_start` (TIMESTAMP)
        - `current_period_end` (TIMESTAMP)
        - `cancel_at_period_end` (BOOLEAN)
        - `created_at`, `updated_at` (TIMESTAMP)
      - `marts.payment_transactions` table with columns:
        - `transaction_id` (SERIAL PRIMARY KEY)
        - `user_id` (INTEGER, FK)
        - `subscription_id` (INTEGER, FK, nullable)
        - `amount` (NUMERIC)
        - `currency` (VARCHAR)
        - `status` (VARCHAR) - 'pending', 'completed', 'failed', 'refunded'
        - `payment_method` (VARCHAR) - 'stripe', 'paypal', etc.
        - `external_transaction_id` (VARCHAR) - payment provider transaction ID
        - `transaction_date` (TIMESTAMP)
        - `metadata` (JSONB, nullable) - additional transaction data
      - `marts.subscription_plans` table with columns:
        - `plan_id` (SERIAL PRIMARY KEY)
        - `plan_name` (VARCHAR) - e.g., 'Free', 'Basic', 'Premium'
        - `plan_type` (VARCHAR) - unique identifier
        - `price_monthly` (NUMERIC)
        - `price_yearly` (NUMERIC, nullable)
        - `max_campaigns` (INTEGER)
        - `max_jobs_per_campaign` (INTEGER, nullable)
        - `features` (JSONB) - feature flags and limits
        - `is_active` (BOOLEAN)
        - `created_at`, `updated_at` (TIMESTAMP)
    - Appropriate indexes and foreign keys
    - Unique constraints where needed
    - Update files:
      - `docker/init/15_add_payment_tables.sql` (migration script)

- [ ] **3.18.3: Integrate Payment Provider (Stripe Recommended)**
  - **Acceptance Criteria:**
    - Install payment provider SDK (e.g., `stripe` Python package)
    - Configure payment provider API keys (stored in environment variables)
    - Create payment service abstraction:
      - `services/payments/payment_service.py` - main payment service
      - `services/payments/stripe_client.py` - Stripe-specific implementation
    - Implement core payment methods:
      - Create customer
      - Create subscription
      - Update subscription
      - Cancel subscription
      - Handle webhooks (payment success, failure, subscription updates)
    - Error handling and logging
    - Update files:
      - `services/payments/payment_service.py` (payment service interface)
      - `services/payments/stripe_client.py` (Stripe implementation)
      - `campaign_ui/requirements.txt` (add stripe dependency)
      - `.env.example` (add Stripe API keys)

- [ ] **3.18.4: Implement Subscription Management Service**
  - **Acceptance Criteria:**
    - Create `services/payments/subscription_service.py`
    - Service methods:
      - `get_user_subscription(user_id)` - get current subscription
      - `create_subscription(user_id, plan_type, payment_method)` - create new subscription
      - `update_subscription(subscription_id, new_plan)` - upgrade/downgrade
      - `cancel_subscription(subscription_id, immediate=False)` - cancel subscription
      - `check_feature_access(user_id, feature)` - check if user has access to feature
      - `get_plan_limits(user_id)` - get plan limits (campaigns, jobs, etc.)
    - Handles subscription status transitions
    - Validates plan limits before allowing actions
    - Update files:
      - `services/payments/subscription_service.py`
      - `services/payments/queries.py` (SQL queries for subscriptions)

- [ ] **3.18.5: Add Payment Routes and UI**
  - **Acceptance Criteria:**
    - Payment/subscription routes in `campaign_ui/app.py`:
      - `/subscription` - view current subscription
      - `/subscription/upgrade` - upgrade subscription page
      - `/subscription/cancel` - cancel subscription
      - `/payment/webhook` - webhook endpoint for payment provider
    - Subscription management page:
      - Shows current plan
      - Shows plan limits and usage
      - Upgrade/downgrade options
      - Billing history
      - Payment method management
    - Checkout flow for new subscriptions
    - Update files:
      - `campaign_ui/app.py` (add payment routes)
      - `campaign_ui/templates/subscription.html` (subscription management page)
      - `campaign_ui/templates/checkout.html` (checkout page)
      - `campaign_ui/static/css/` (subscription page styles)

- [ ] **3.18.6: Implement Feature Gating**
  - **Acceptance Criteria:**
    - Add feature checks before allowing actions:
      - Campaign creation (check max_campaigns limit)
      - Job extraction (check plan allows extraction)
      - ChatGPT enrichment (check if plan includes AI features)
      - Cover letter generation (check if plan includes AI features)
      - Document storage limits
    - Show upgrade prompts when users hit limits
    - Graceful degradation (show limits, suggest upgrade)
    - Update files:
      - `campaign_ui/app.py` (add feature checks to routes)
      - `services/campaign_management/campaign_service.py` (check limits before creating campaigns)
      - `services/payments/subscription_service.py` (feature access methods)
      - `campaign_ui/templates/` (add upgrade prompts/notices)

- [ ] **3.18.7: Add Payment Webhook Handler**
  - **Acceptance Criteria:**
    - Webhook endpoint handles payment provider events:
      - `payment_intent.succeeded` - payment successful
      - `payment_intent.payment_failed` - payment failed
      - `customer.subscription.created` - subscription created
      - `customer.subscription.updated` - subscription updated
      - `customer.subscription.deleted` - subscription cancelled
      - `invoice.payment_succeeded` - recurring payment succeeded
      - `invoice.payment_failed` - recurring payment failed
    - Webhook handler updates database accordingly
    - Webhook signature verification for security
    - Idempotent webhook processing (handle duplicate events)
    - Error handling and logging
    - Update files:
      - `campaign_ui/app.py` (webhook route)
      - `services/payments/webhook_handler.py` (webhook processing logic)

- [ ] **3.18.8: Add Subscription Status to User Interface**
  - **Acceptance Criteria:**
    - Display subscription status in user profile/account page
    - Show plan limits and current usage (e.g., "3/5 campaigns used")
    - Show upgrade prompts when approaching limits
    - Display billing information and next billing date
    - Show subscription expiration warnings
    - Update files:
      - `campaign_ui/templates/account_management.html` (add subscription section)
      - `campaign_ui/app.py` (add subscription data to account page)

- [ ] **3.18.9: Add Tests for Payment Integration**
  - **Acceptance Criteria:**
    - Unit tests for payment service methods (with mocked payment provider)
    - Unit tests for subscription service
    - Integration tests for payment flow (using test payment provider)
    - Webhook handler tests
    - Feature gating tests
    - Update files:
      - `tests/unit/test_payment_service.py`
      - `tests/unit/test_subscription_service.py`
      - `tests/integration/test_payment_flow.py`

### Social Media Authentication

- [ ] **3.17.1: Add OAuth Provider Support to Database Schema**
  - **Acceptance Criteria:**
    - Migration script `docker/init/10_add_oauth_provider_support.sql` adds OAuth columns to `marts.users`:
      - `oauth_provider` (VARCHAR, nullable) - e.g., 'google', 'facebook'
      - `oauth_provider_id` (VARCHAR, nullable) - unique ID from OAuth provider
      - `password_hash` becomes nullable (OAuth users don't have passwords)
      - Unique constraint on `(oauth_provider, oauth_provider_id)` for OAuth accounts
      - Index on `oauth_provider_id` for lookup performance
    - Migration is idempotent and handles existing data
    - Update files:
      - `docker/init/10_add_oauth_provider_support.sql` (migration script)
      - `services/auth/queries.py` (update queries to handle nullable password_hash and OAuth fields)

- [ ] **3.17.2: Install and Configure Flask-Dance for OAuth**
  - **Acceptance Criteria:**
    - Add `flask-dance` to requirements (e.g., `campaign_ui/requirements.txt` or project-level `requirements.txt`)
    - Configure Flask-Dance with at least one OAuth provider (Google recommended as primary)
    - OAuth provider credentials stored in environment variables (client ID, client secret)
    - OAuth configuration follows Flask-Dance patterns
    - Update files:
      - `campaign_ui/requirements.txt` (add flask-dance dependency)
      - `campaign_ui/app.py` (configure Flask-Dance OAuth blueprints)
      - `.env.example` (add OAuth credential examples)

- [ ] **3.17.3: Extend AuthService for OAuth Authentication**
  - **Acceptance Criteria:**
    - Create or update `services/auth/oauth_service.py` for OAuth user management
    - Service methods:
      - `get_or_create_oauth_user()` - lookup or create user from OAuth provider data
      - `link_oauth_account()` - link OAuth account to existing user (optional)
      - `unlink_oauth_account()` - remove OAuth link (optional)
    - Handles OAuth provider user data extraction (email, username, etc.)
    - Creates users with appropriate defaults when OAuth user doesn't exist
    - Updates `services/auth/user_service.py` queries to support OAuth lookup
    - Error handling for OAuth authentication failures
    - Update files:
      - `services/auth/oauth_service.py` (new OAuth service)
      - `services/auth/user_service.py` (add OAuth lookup methods)
      - `services/auth/queries.py` (add OAuth-related queries)

- [ ] **3.17.4: Add OAuth Routes to Flask Application**
  - **Acceptance Criteria:**
    - OAuth login routes for each configured provider (e.g., `/login/google`, `/login/github`)
    - OAuth callback routes handle provider responses
    - Successful OAuth login creates/updates user and logs them in via Flask-Login
    - OAuth routes handle errors gracefully (user rejection, provider errors)
    - OAuth login integrates with existing Flask-Login session management
    - Update files:
      - `campaign_ui/app.py` (add OAuth routes and Flask-Dance blueprint integration)

- [ ] **3.17.5: Update Login UI with OAuth Options**
  - **Acceptance Criteria:**
    - Login page displays OAuth provider buttons (e.g., "Sign in with Google")
    - OAuth buttons styled consistently with existing UI
    - Traditional username/password login remains available
    - Clear visual separation between OAuth and traditional login
    - Registration page also shows OAuth options (optional but recommended)
    - Update files:
      - `campaign_ui/templates/login.html` (add OAuth buttons)
      - `campaign_ui/templates/register.html` (optional: add OAuth options)
      - `campaign_ui/static/css/` (style OAuth buttons if needed)

- [ ] **3.17.6: Update User Service Queries for OAuth Support**
  - **Acceptance Criteria:**
    - `GET_USER_BY_OAUTH` query for OAuth provider lookup
    - `INSERT_OAUTH_USER` query for creating OAuth users (password_hash nullable)
    - `UPDATE_USER_OAUTH_LINK` query for linking OAuth accounts (if supporting linking)
    - All user queries handle nullable password_hash appropriately
    - Queries tested with both traditional and OAuth users
    - Update files:
      - `services/auth/queries.py` (add OAuth queries)
      - `services/auth/user_service.py` (update methods to use OAuth queries)

- [ ] **3.17.7: Add Tests for OAuth Authentication**
  - **Acceptance Criteria:**
    - Unit tests for OAuth service methods (mocking OAuth provider responses)
    - Integration tests for OAuth login flow (using test OAuth provider or mocks)
    - Tests verify OAuth user creation and lookup
    - Tests verify OAuth login creates Flask-Login session
    - Tests handle edge cases (duplicate emails across providers, etc.)
    - Update files:
      - `tests/unit/test_oauth_service.py` (unit tests)
      - `tests/integration/test_oauth_authentication.py` (integration tests)

---

## Phase 4: DigitalOcean Production Deployment

### Development Environment (Local)

- [x] **4.0: Local Development Environment Setup**
  - **Status**: Completed - Local Docker Compose environment is fully functional
  - **Acceptance Criteria:**
    - Docker Compose stack runs locally with all services
    - PostgreSQL database accessible on localhost:5432
    - Airflow UI accessible on localhost:8080
    - Campaign UI accessible on localhost:5000
    - All services can connect to local database
    - Environment variables configured via `.env` file
    - Can run full ETL pipeline locally
  - **Files**: `docker-compose.yml`, `.env.example`, `README.md`

### Environment Configuration Management

- [ ] **4.1: Create Environment-Specific Configuration Files**
  - **Acceptance Criteria:**
    - Create `.env.development` for local development
    - Create `.env.staging` for staging environment
    - Create `.env.production` for production environment
    - Each file contains:
      - Database connection strings (different for each env)
      - API keys (can be same or different)
      - Airflow configuration
      - SMTP settings
      - Flask secret keys (different per environment)
    - `.env.development` is git-ignored but `.env.example` documents all variables
    - `.env.staging` and `.env.production` stored securely (not in git)
    - Documentation on how to manage secrets per environment
  - **Update files:**
    - Create `.env.development` (git-ignored)
    - Create `.env.staging` template
    - Create `.env.production` template
    - Update `.env.example` with all environment variables
    - Create `project_documentation/deployment-environments.md` with environment setup guide

- [ ] **4.2: Set Up Environment Variable Management**
  - **Acceptance Criteria:**
    - Environment variables loaded from appropriate `.env` file based on `ENVIRONMENT` variable
    - Docker Compose uses environment-specific files
    - Services can access environment variables correctly
    - Secrets are never committed to git
    - Documentation on where to store production secrets (DigitalOcean App Platform secrets, or manual setup)
  - **Update files:**
    - `docker-compose.yml` (support multiple env files)
    - `campaign_ui/app.py` (load env vars correctly)
    - `airflow/dags/task_functions.py` (use env vars)
    - Create `scripts/load_env.sh` helper script

### Staging Environment Setup

- [ ] **4.3: Create DigitalOcean Staging Droplet**
  - **Acceptance Criteria:**
    - DigitalOcean droplet created:
      - Size: Basic plan (1GB RAM, 1 vCPU, 25GB SSD) - $12/month
      - OS: Ubuntu 22.04 LTS
      - Region: Choose closest to users
      - SSH key configured for access
    - Droplet accessible via SSH
    - Firewall configured (UFW or DigitalOcean Firewall):
      - Allow SSH (port 22)
      - Allow HTTP (port 80)
      - Allow HTTPS (port 443)
      - Allow Airflow UI (port 8080) - restrict to specific IPs or VPN
      - Allow Campaign UI (port 5000) - restrict to specific IPs or VPN
    - Hostname set (e.g., `staging.jobsearch.example.com`)
    - Basic security hardening completed (disable root login, fail2ban, etc.)
  - **Documentation**: Create `project_documentation/deployment-staging.md`

- [ ] **4.4: Set Up DigitalOcean Managed PostgreSQL for Staging**
  - **Acceptance Criteria:**
    - DigitalOcean Managed PostgreSQL database created:
      - Plan: Basic (1GB RAM, 10GB storage, 1 vCPU) - $15/month
      - Version: PostgreSQL 15 (matches local)
      - Region: Same as droplet
      - Database name: `job_search_staging`
    - Connection pooler enabled (optional but recommended)
    - Automated backups enabled (daily backups, 7-day retention)
    - Firewall rules configured to allow access from staging droplet only
    - Connection string documented and stored securely
    - Test connection from staging droplet
  - **Update files:**
    - `project_documentation/deployment-staging.md` (add database setup)
    - Store connection string in DigitalOcean App Platform secrets or environment variables

- [ ] **4.5: Install Docker and Docker Compose on Staging Droplet**
  - **Acceptance Criteria:**
    - Docker Engine installed (latest stable version)
    - Docker Compose v2 installed
    - Docker daemon configured to start on boot
    - Non-root user added to docker group (for running docker without sudo)
    - Docker Compose can run successfully
    - Test with simple container (e.g., `docker run hello-world`)
  - **Documentation**: Add installation steps to `project_documentation/deployment-staging.md`

- [ ] **4.6: Deploy Application to Staging Droplet**
  - **Acceptance Criteria:**
    - Code deployed to staging droplet (via git clone or CI/CD)
    - `.env.staging` file created on droplet with correct values
    - Docker Compose stack started on staging:
      - Airflow webserver and scheduler
      - Campaign UI
      - All services connect to staging database
    - Services accessible:
      - Airflow UI: `http://staging-droplet-ip:8080` (or via domain)
      - Campaign UI: `http://staging-droplet-ip:5000` (or via domain)
    - All services healthy (health checks pass)
    - Can trigger DAG manually and it completes successfully
  - **Update files:**
    - Create `scripts/deploy-staging.sh` deployment script
    - Update `docker-compose.staging.yml` (if needed for staging-specific config)

- [ ] **4.7: Set Up Staging Database Schema**
  - **Acceptance Criteria:**
    - All schemas created on staging database: `raw`, `staging`, `marts`
    - All tables created via migration scripts or dbt
    - Test data loaded (optional, for testing)
    - dbt can connect and run models successfully
    - Verify data flows through all layers (raw â†’ staging â†’ marts)
  - **Update files:**
    - Run `docker/init/01_create_schemas.sql` on staging database
    - Run `docker/init/02_create_tables.sql` on staging database
    - Run all migration scripts in order
    - Test dbt connection: `dbt debug --profiles-dir . --profile job_search_platform`

- [ ] **4.8: Configure Staging Domain and SSL**
  - **Acceptance Criteria:**
    - Domain or subdomain configured for staging (e.g., `staging.jobsearch.example.com`)
    - DNS A record points to staging droplet IP
    - Nginx or Caddy reverse proxy installed and configured:
      - Routes `/` to Campaign UI (port 5000)
      - Routes `/airflow` to Airflow webserver (port 8080)
      - SSL certificate via Let's Encrypt (automatic renewal)
    - HTTPS working for both services
    - HTTP redirects to HTTPS
  - **Update files:**
    - Create `infra/nginx/staging.conf` (Nginx config)
    - Create `scripts/setup-ssl-staging.sh` (SSL setup script)
    - Update `project_documentation/deployment-staging.md`

### Production Environment Setup

- [ ] **4.9: Create DigitalOcean Production Droplet**
  - **Acceptance Criteria:**
    - DigitalOcean droplet created:
      - Size: Regular (4GB RAM, 2 vCPU, 80GB SSD) - $24/month
      - OS: Ubuntu 22.04 LTS
      - Region: Choose closest to users (can be different from staging)
      - SSH key configured for access
      - Backup enabled (weekly snapshots)
    - Droplet accessible via SSH
    - Firewall configured (more restrictive than staging):
      - Allow SSH (port 22) - restrict to specific IPs or VPN
      - Allow HTTP (port 80)
      - Allow HTTPS (port 443)
      - Airflow UI (port 8080) - restrict to specific IPs or VPN only
      - Campaign UI (port 5000) - behind reverse proxy only
    - Hostname set (e.g., `app.jobsearch.example.com`)
    - Security hardening completed:
      - Fail2ban configured
      - Root login disabled
      - SSH key-only authentication
      - Unnecessary services disabled
      - Regular security updates automated
  - **Documentation**: Create `project_documentation/deployment-production.md`

- [ ] **4.10: Set Up DigitalOcean Managed PostgreSQL for Production**
  - **Acceptance Criteria:**
    - DigitalOcean Managed PostgreSQL database created:
      - Plan: Basic (1GB RAM, 10GB storage, 1 vCPU) - $15/month
        - Can upgrade to Regular (2GB RAM, 25GB storage, 1 vCPU) - $25/month if needed
      - Version: PostgreSQL 15 (matches staging and local)
      - Region: Same as production droplet
      - Database name: `job_search_production`
    - Connection pooler enabled (recommended for production)
    - Automated backups enabled:
      - Daily backups
      - 7-day retention (can increase if needed)
    - Point-in-time recovery enabled (if available on plan)
    - Firewall rules configured to allow access from production droplet only
    - Connection string stored securely (DigitalOcean App Platform secrets or environment variables)
    - Test connection from production droplet
    - SSL/TLS connection enforced
  - **Update files:**
    - `project_documentation/deployment-production.md` (add database setup)
    - Document connection string management

- [ ] **4.11: Install Docker and Docker Compose on Production Droplet**
  - **Acceptance Criteria:**
    - Docker Engine installed (latest stable version)
    - Docker Compose v2 installed
    - Docker daemon configured to start on boot
    - Non-root user added to docker group
    - Docker Compose can run successfully
    - Docker logging configured (log rotation, size limits)
    - Test with simple container
  - **Documentation**: Add to `project_documentation/deployment-production.md`

- [ ] **4.12: Deploy Application to Production Droplet**
  - **Acceptance Criteria:**
    - Code deployed to production droplet (via git clone or CI/CD)
    - `.env.production` file created on droplet with correct values
    - All secrets stored securely (not in git)
    - Docker Compose stack started on production:
      - Airflow webserver and scheduler
      - Campaign UI
      - All services connect to production database
    - Services accessible:
      - Airflow UI: `http://production-droplet-ip:8080` (or via domain, restricted access)
      - Campaign UI: `http://production-droplet-ip:5000` (or via domain)
    - All services healthy (health checks pass)
    - Can trigger DAG manually and it completes successfully
    - Services configured to restart automatically on failure
  - **Update files:**
    - Create `scripts/deploy-production.sh` deployment script
    - Update `docker-compose.production.yml` (if needed for production-specific config)
    - Add systemd service files for auto-restart (optional)

- [ ] **4.13: Set Up Production Database Schema**
  - **Acceptance Criteria:**
    - All schemas created on production database: `raw`, `staging`, `marts`
    - All tables created via migration scripts or dbt
    - Indexes and constraints verified
    - dbt can connect and run models successfully
    - Initial data migration completed (if migrating from local/staging)
    - Data validation completed (row counts, referential integrity)
  - **Update files:**
    - Run all migration scripts in order on production database
    - Test dbt connection and models
    - Create `scripts/migrate-data-to-production.sh` (if migrating existing data)

- [ ] **4.14: Configure Production Domain and SSL**
  - **Acceptance Criteria:**
    - Production domain configured (e.g., `app.jobsearch.example.com`)
    - DNS A record points to production droplet IP
    - Nginx or Caddy reverse proxy installed and configured:
      - Routes `/` to Campaign UI (port 5000)
      - Routes `/airflow` to Airflow webserver (port 8080)
      - SSL certificate via Let's Encrypt (automatic renewal)
      - Security headers configured (HSTS, CSP, etc.)
    - HTTPS working for both services
    - HTTP redirects to HTTPS
    - SSL certificate auto-renewal tested
  - **Update files:**
    - Create `infra/nginx/production.conf` (Nginx config with security headers)
    - Create `scripts/setup-ssl-production.sh` (SSL setup script)
    - Update `project_documentation/deployment-production.md`

### Backup and Disaster Recovery

- [ ] **4.15: Set Up Automated Database Backups**
  - **Acceptance Criteria:**
    - DigitalOcean managed database backups are enabled (daily, 7-day retention)
    - Additional backup strategy implemented:
      - Daily pg_dump backups to DigitalOcean Spaces (S3-compatible)
      - Backup retention policy (e.g., keep 30 daily, 12 monthly)
      - Backup files compressed (gzip)
    - Backup script runs via cron or Airflow DAG
    - Backup restoration tested and documented
    - Backup monitoring (alert if backup fails)
  - **Update files:**
    - Create `scripts/backup-database.sh` (pg_dump script)
    - Create `scripts/restore-database.sh` (restore script)
    - Create Airflow DAG `backup_database_daily.py` (optional)
    - Update `project_documentation/deployment-production.md` with backup procedures

- [ ] **4.16: Set Up Application Data Backups**
  - **Acceptance Criteria:**
    - Campaign UI uploads (resumes, cover letters) backed up to DigitalOcean Spaces
    - Airflow logs backed up (optional, can be ephemeral)
    - Backup script runs daily via cron
    - Backup retention policy configured
    - Backup restoration tested
  - **Update files:**
    - Create `scripts/backup-uploads.sh` (backup uploads directory)
    - Create `scripts/restore-uploads.sh` (restore script)
    - Update `project_documentation/deployment-production.md`

- [ ] **4.17: Create Disaster Recovery Plan**
  - **Acceptance Criteria:**
    - Document recovery procedures for:
      - Database failure (restore from backup)
      - Droplet failure (recreate from snapshot or backup)
      - Data corruption (point-in-time recovery)
      - Complete system failure (full restore procedure)
    - Recovery time objectives (RTO) and recovery point objectives (RPO) defined
    - Recovery procedures tested (at least once)
    - Contact information and escalation procedures documented
  - **Update files:**
    - Create `project_documentation/disaster-recovery-plan.md`

### Monitoring and Logging

- [ ] **4.18: Set Up Application Monitoring**
  - **Acceptance Criteria:**
    - Monitoring solution configured (e.g., DigitalOcean Monitoring, Prometheus, or Datadog):
      - CPU, memory, disk usage monitoring
      - Database connection monitoring
      - Service health checks
      - Airflow DAG run status monitoring
    - Alerts configured for:
      - High CPU/memory usage
      - Disk space low
      - Database connection failures
      - DAG failures
      - Service downtime
    - Alerts sent to email or Slack
    - Dashboard created for key metrics
  - **Update files:**
    - Create `infra/monitoring/` directory with monitoring configs
    - Update `project_documentation/deployment-production.md` with monitoring setup

- [ ] **4.19: Set Up Centralized Logging**
  - **Acceptance Criteria:**
    - Log aggregation configured (e.g., ELK stack, Loki, or cloud logging):
      - Application logs (Campaign UI, services)
      - Airflow logs
      - System logs (syslog)
    - Log retention policy configured (e.g., 30 days)
    - Log search and filtering available
    - Error logs highlighted and alertable
  - **Update files:**
    - Create `infra/logging/` directory with logging configs
    - Update Docker Compose to use logging driver
    - Update `project_documentation/deployment-production.md`

- [ ] **4.20: Set Up Uptime Monitoring**
  - **Acceptance Criteria:**
    - Uptime monitoring service configured (e.g., UptimeRobot, Pingdom, or DigitalOcean Monitoring):
      - Campaign UI endpoint monitored (HTTP/HTTPS)
      - Airflow UI endpoint monitored (if publicly accessible)
      - Database connectivity monitored
    - Alerts configured for downtime
    - Status page created (optional, for public visibility)
  - **Update files:**
    - Document uptime monitoring setup in `project_documentation/deployment-production.md`

### CI/CD and Deployment Automation

- [ ] **4.21: Set Up GitHub Actions for Deployment**
  - **Acceptance Criteria:**
    - GitHub Actions workflow created for:
      - Staging deployment (on merge to `staging` branch)
      - Production deployment (on merge to `main` branch or manual trigger)
    - Workflow steps:
      - Run tests (linting, unit tests, integration tests)
      - Build Docker images (if needed)
      - Deploy to appropriate environment
      - Run database migrations (if needed)
      - Health check after deployment
    - Secrets stored in GitHub Secrets:
      - SSH private key for droplet access
      - Database connection strings
      - API keys
    - Deployment can be triggered manually or automatically
    - Rollback procedure documented
  - **Update files:**
    - Create `.github/workflows/deploy-staging.yml`
    - Create `.github/workflows/deploy-production.yml`
    - Update `project_documentation/deployment-production.md` with CI/CD setup

- [ ] **4.22: Create Deployment Scripts**
  - **Acceptance Criteria:**
    - Deployment script for staging:
      - Connects to staging droplet via SSH
      - Pulls latest code
      - Updates environment variables
      - Runs database migrations (if needed)
      - Restarts services
      - Verifies deployment
    - Deployment script for production:
      - Same as staging but with additional safety checks
      - Pre-deployment backup
      - Health checks before and after
      - Rollback capability
    - Scripts are idempotent (safe to run multiple times)
    - Scripts log all actions
  - **Update files:**
    - Create `scripts/deploy-staging.sh`
    - Create `scripts/deploy-production.sh`
    - Create `scripts/rollback-production.sh`
    - Update `project_documentation/deployment-production.md`

### Security Hardening

- [ ] **4.23: Implement Production Security Best Practices**
  - **Acceptance Criteria:**
    - Firewall rules configured (UFW or DigitalOcean Firewall):
      - Only necessary ports open
      - SSH restricted to specific IPs or VPN
      - Services not publicly accessible unless needed
    - Fail2ban configured for SSH protection
    - Regular security updates automated (unattended-upgrades)
    - Secrets management:
      - All secrets in environment variables (not hard-coded)
      - Secrets stored securely (DigitalOcean App Platform secrets or encrypted)
      - Secrets rotation procedure documented
    - SSL/TLS configured for all connections:
      - Database connections use SSL
      - Application uses HTTPS
    - Security headers configured (HSTS, CSP, X-Frame-Options, etc.)
    - Database access restricted (only from application droplet)
  - **Update files:**
    - Create `scripts/security-hardening.sh`
    - Update `infra/nginx/production.conf` with security headers
    - Create `project_documentation/security-checklist.md`

- [ ] **4.24: Set Up Database Security**
  - **Acceptance Criteria:**
    - Database users follow principle of least privilege:
      - Application user has only necessary permissions
      - Read-only user for analytics (if needed)
      - No superuser access from application
    - Database connections use SSL/TLS
    - Database firewall rules restrict access to application droplet only
    - Database credentials rotated regularly
    - Database audit logging enabled (if available)
  - **Update files:**
    - Create `scripts/setup-database-users.sql`
    - Update `project_documentation/deployment-production.md`

### BI Integration

- [ ] **4.25: Connect Tableau to Production Database**
  - **Acceptance Criteria:**
    - Read-only database user created for Tableau
    - Tableau connection configured to DigitalOcean Managed PostgreSQL
    - Connection uses SSL/TLS
    - Connection is stable and performant
    - Can browse marts schema tables
    - Connection tested from Tableau Desktop/Server
  - **Update files:**
    - Create `scripts/create-tableau-user.sql` (read-only user)
    - Update `project_documentation/deployment-production.md` with Tableau connection details

- [ ] **4.26: Build Initial Tableau Dashboards**
  - **Acceptance Criteria:**
    - Dashboard for skills trends (most demanded skills over time)
    - Dashboard for salary trends (by location, company, role)
    - Dashboard for job volumes (postings over time, by company)
    - Dashboard for company ratings distribution
    - Dashboard for ranking score distributions
    - Dashboards refresh from live production database
    - Dashboards performant (queries optimized)
  - **Update files:**
    - Document dashboard requirements in `project_documentation/tableau-dashboards.md`

### Operational Runbooks

- [ ] **4.27: Create Operational Runbooks**
  - **Acceptance Criteria:**
    - Documentation for common operations:
      - How to manually trigger DAGs
      - How to investigate failed DAG runs
      - How to update campaign preferences
      - How to access logs (application, Airflow, system)
      - How to restart services
      - How to scale services (upgrade droplet)
      - How to perform database migrations
      - How to restore from backup
      - How to rollback deployment
    - Runbooks are clear, step-by-step, and actionable
    - Runbooks include troubleshooting steps
    - Contact information and escalation procedures
  - **Update files:**
    - Create `project_documentation/operational-runbooks.md`

---

## Post-Implementation

- [ ] **5.1: Performance Optimization**
  - Review and optimize slow queries
  - Add indexes where needed
  - Optimize API call patterns
  - Cache frequently accessed data if beneficial

- [ ] **5.2: Documentation Finalization**
  - Architecture diagrams updated
  - API documentation for services
  - User guide for profile management UI
  - Developer onboarding guide

- [ ] **5.3: Load Testing**
  - Test pipeline with multiple profiles
  - Test with high job volumes
  - Verify system handles peak loads gracefully

---

## Notes

- Tasks within each phase should generally be completed in order, but some can be parallelized
- Acceptance criteria should be verified before marking tasks complete
- Consider creating GitHub issues or project board items from this TODO list for tracking
- Adjust timeline estimates based on your experience level and available time

