---
alwaysApply: true
---


## Sections in This File
1. [**Agent Workflow**](#agent-workflow)
2. [**Project Context**](#project-context)
3. [**Core Principles**](#core-principles)
4. [**User Request Handling**](#user-request-handling)
5. [**Code Quality Standards**](#code-quality-standards)
6. [**Project Structure**](#project-structure)
7. [**Architecture Patterns**](#architecture-patterns)
8. [**Testing Standards**](#testing-standards)
9. [**Version Control Strategy**](#version-control-strategy)
   - [**GitHub Operations via MCP**](#github-operations-via-mcp)
10. [**Code Review Checklist**](#code-review-checklist)
11. [**Reference Documents**](#reference-documents)

## Agent Workflow

This section defines how to use **Cursor chat** to work on Linear tasks through the development lifecycle using MCP (Model Context Protocol) for task management.

### IMPORTANT: Chat-Based Workflow

**This project uses Cursor chat to request work on Linear tasks.** Each Linear task is worked on in a **separate Git worktree** for complete isolation.

**How to Use**

1. **Open Cursor chat** and request work on a Linear issue
2. **Agent checks issue status** via Linear MCP to determine the appropriate phase
3. **Agent follows the workflow** for that phase
4. **Agent updates Linear status** as work progresses

**Key Requirements**

- **Worktrees**: Each Linear issue gets its own worktree in `.worktrees/linear-{issue-id}-{description}/`
- **MCP Integration**: Agent uses Linear MCP tools to read and update tasks
- **Isolation**: One worktree = One branch = One staging slot per issue
- **NO DIRECT PUSH TO MAIN**: (MANDATORY) Agents must NEVER push commits directly to the `main` branch. All changes MUST be committed to a dedicated feature branch (`linear-{issue-id}-{short-description}`) and merged into `main` ONLY via a Pull Request (PR).
- **Staging-First Fixing**: When fixing issues (CI failures, QA bugs, code review feedback), apply changes ONLY to the worktree/branch associated with the staging slot. DO NOT apply fixes to the local main branch or any environment not linked to the staging slot.
- **Branch naming**: Use `linear-{issue-id}-{short-description}` NOT `cursor/...`
- **Comments**: Use our structured templates, NOT Cursor's default format
- **Status updates**: Agent MUST update Linear status at each phase via MCP only when the phase completes successfully
- **On failure**: If the agent cannot complete the phase, do **not** move the card to the next phase. Leave a comment on the Linear issue with reasons and explanations of what went wrong. Keep the issue in its current status.

### Common Checklist (All Phases)

**When working on a Linear task via Cursor chat, the agent MUST:**

- [ ] Read Linear issue via Linear MCP to get current status and details
- [ ] Determine the appropriate phase based on issue status
- [ ] Use Linear MCP to update issue status as work progresses (only when work succeeds)
- [ ] Use Linear MCP to add completion comments using structured templates
- [ ] Use GitHub MCP for all PR operations (create, read, review, merge)
- [ ] Use DigitalOcean MCP for environment control and deployment (droplets, databases, actions)
- [ ] Follow branch naming: `linear-{issue-id}-{short-description}`
- [ ] Leave completion comment using appropriate template
- [ ] Update Linear status at phase completion **only if the phase completed successfully**
- [ ] **If the phase fails**: Do **not** change the issue status. Leave a comment on the Linear issue with reasons and explanations of what went wrong (see Failure Comment Template below).

### Failure Comment Template

When a phase cannot be completed, leave a comment on the Linear issue using this structure. **Do not move the card to the next phase.**

```markdown
**Phase failed: [Development / Code Review / QA / Deploy / CI Fix]**

**What went wrong:**
- {Clear description of the failure}

**Reason(s):**
- {Explanation of why it failed}

**What was attempted:**
- {Brief summary of steps already taken}

**Next steps / What's needed:**
- {What the user or a retry needs to do to unblock}
```

**Status:** Issue remains in [current status]. No status change was made.

### Quick Start Checklists

#### Development Phase

**When requested**: "Work on Linear issue ABC-123" or "Develop Linear issue ABC-123"

**Agent checks**: Issue status should be `Todo` or `Fixes needed`

- [ ] Read Linear issue via Linear MCP to get issue details
- [ ] Verify issue status is `Todo` or `Fixes needed` (if not, inform user)
- [ ] Update Linear status to `In Progress` via Linear MCP
- [ ] Create worktree: `linear-{issue-id}-{short-description}` (MUST use `linear-` prefix) or use the existing in case of fixes
- [ ] Change to worktree directory
- [ ] Implement changes following cursor rules
- [ ] Run tests and linting (from worktree directory)
- [ ] Push branch
- [ ] **MANDATORY**: Wait for CI to complete and verify status (see [CI Status Checking After Push](#ci-status-checking-after-push))
- [ ] Create PR via GitHub MCP (NOT just pushing)
- [ ] **MANDATORY**: Update Linear status to `Code Review` (exact status name) via Linear MCP
- [ ] Leave completion comment using template below

**Completion Comment Template:**
```markdown
**Development Complete**
- Branch: `linear-{issue-id}-{description}`
- PR: {link to PR}
- Changes: {summary of what was done}
- Tests: {test results}
- CI Status: {PASSED/FAILED with details}
- **Status updated to: Code Review**
```

#### Code Review Phase

**When requested**: "Review Linear issue ABC-123" or "Review the PR for ABC-123"

**Agent checks**: Issue status should be `Code Review`

- [ ] Query Linear for issues with status `Code Review` via Linear MCP
- [ ] Get issue details to find PR link from comments
- [ ] Read PR using GitHub MCP tools
- [ ] Review against Code Review Checklist (see [Code Review Checklist](#code-review-checklist) in this file for full checklist)
- [ ] Add inline comments via GitHub MCP if issues found
- [ ] If approved:
  - [ ] Update Linear status to `QA` via Linear MCP
  - [ ] Approve PR via GitHub MCP
- [ ] If changes needed:
  - [ ] Update Linear status to `Fixes needed` via Linear MCP
  - [ ] Request changes on PR via GitHub MCP
- [ ] Leave completion comment using template below

**Completion Comment Template:**
```markdown
**Code Review Complete**
- Status: APPROVED / CHANGES_REQUESTED
- Issues: [list if any]
- Status updated to: QA (if approved) or Fixes needed (if changes needed)
```

#### QA Phase

**When requested**: "Do QA for Linear issue ABC-123" or "Test Linear issue ABC-123"

**Agent checks**: Issue status should be `QA`

- [ ] Query Linear for issues with status `QA` via Linear MCP
- [ ] Get issue details to find branch name and PR link
- [ ] Assess change type to determine verification needed
- [ ] Claim staging slot (slots 1-9, update `staging-slots.md`; slot 10 is reserved for production)
- [ ] Deploy to staging from worktree (if exists) or branch
  - [ ] Use DigitalOcean MCP to check droplet status and database cluster health if needed
  - [ ] Use deployment scripts (`deploy-staging.sh`) or DigitalOcean MCP for environment management
- [ ] Perform relevant verification based on change type:
  - [ ] Frontend changes → UI verification with screenshots
  - [ ] Backend API changes → API endpoint testing
  - [ ] Database/dbt changes → Data verification
  - [ ] Service logic changes → Functional testing
- [ ] If passed: Update Linear status to `Ready to Deploy` via Linear MCP
- [ ] If failed: Update Linear status to `Fixes needed` via Linear MCP, release staging slot
- [ ] Leave completion comment using template below

**Completion Comment Template:**
```markdown
**QA Verification Complete**

**Staging Environment**
- Slot: staging-N (STILL ALLOCATED - will be released after merge)
- Branch: linear-{issue-id}-{description}
- URL: https://staging-N.jobsearch.example.com

**Change Type**: [Frontend / Backend API / Database / Service Logic]

**Verification Performed**:
- [x] [Describe what was verified]
- [x] [Describe what was verified]

**Evidence**:
[For UI changes: Screenshots attached]
[For API changes: Request/response samples]
[For DB changes: Query results]

**Result**: PASSED / FAILED
**Status updated to**: Ready to Deploy (if passed) or Fixes needed (if failed)
```

#### Deploy Phase

**When requested**: "Deploy Linear issue ABC-123" or "Merge Linear issue ABC-123"

**Agent checks**: Issue status should be `Ready to Deploy`

**CRITICAL**: This phase is responsible for cleanup: releasing staging slot, removing worktree, and deleting branch after merge.

- [ ] Query Linear for issues with status `Ready to Deploy` via Linear MCP
- [ ] Get issue details to find PR, branch, and staging slot info
- [ ] Verify PR is approved and CI passing
- [ ] Read QA comment to get staging slot number
- [ ] Merge PR via GitHub MCP
- [ ] Monitor CI after merge (poll until terminal state)
- [ ] For production deployment (slot 10): Use `./scripts/deploy-production.sh` or DigitalOcean MCP to manage deployment
- [ ] Use DigitalOcean MCP to verify droplet and database cluster status if needed
- [ ] If CI passes:
  - [ ] Release staging slot (update `staging-slots.md`)
  - [ ] Remove worktree: `git worktree remove .worktrees/linear-{issue-id}-{description}` (from project root)
  - [ ] Delete remote branch via GitHub MCP
  - [ ] Update Linear status to `Done` via Linear MCP
- [ ] If CI fails:
  - [ ] Add comment to Linear issue: "CI failed after merge. Need to investigate."
  - [ ] Update status to appropriate status (may need manual intervention)
- [ ] Leave completion comment using template below

**Completion Comment Template:**
```markdown
**Deployment Complete**
- PR merged: {link}
- CI status: PASSED
- Staging slot: staging-N (RELEASED)
- Branch: {name} (DELETED)
- Status: Done
```

#### CI Fix Phase

**When requested**: "Fix CI for Linear issue ABC-123" or "Fix the CI failures"

**Agent checks**: Issue has CI failure comment or PR has failed CI status

- [ ] Query Linear for issues with recent comments mentioning "CI failed" or check PRs with failed CI
- [ ] Get Linear issue via MCP to find branch and worktree location
- [ ] Extract errors using `.github/scripts/report_ci_errors.py`
- [ ] Analyze failures (lint errors, test failures, dbt failures)
- [ ] Work in the worktree (or checkout branch if worktree was removed)
- [ ] Push fixes to the branch
- [ ] Monitor CI again until passing
- [ ] Once passing, add comment to Linear issue: "CI fixes applied and passing. Ready for deployment."
- [ ] If issue is in `Ready to Deploy`: Status remains, Deploy phase will retry
- [ ] If issue is in `Code Review`: Status remains, Review phase can re-review

### Staging-First Fixing

When an agent is tasked with fixing something (e.g., CI failures, QA bugs, or code review feedback), it must strictly adhere to the staging-first principle:

1. **Locate the Correct Worktree**: Identify the worktree/branch associated with the Linear issue and the staging slot where the issue is being tested.
2. **Apply Changes in Worktree**: All fixes, adjustments, and experiments must be performed within that specific worktree.
3. **No Local Branch Fixes**: Never apply fixes directly to the local `main` branch or any branch not currently assigned to a staging slot for that task.
4. **Push and Verify**: Once the fix is applied in the worktree, push the changes to the remote branch to trigger CI and update the staging environment.

### Status Transitions

**IMPORTANT**: Use exact status names as shown below when updating Linear issues via MCP.

```
Todo → In Progress → Code Review → QA → Ready to Deploy → Done
                  ↑                    ↓
                  └── Fixes needed ←───┘
```

| From Status | To Status | Triggered By |
|-------------|-----------|--------------|
| `Todo` | `In Progress` | Development phase starts work |
| `In Progress` | `Code Review` | Development phase completes, creates PR, **CI passes** |
| `Code Review` | `QA` | Code review phase approves |
| `Code Review` | `Fixes needed` | Code review phase requests changes |
| `QA` | `Ready to Deploy` | QA phase verifies successfully |
| `QA` | `Fixes needed` | QA phase finds issues |
| `Ready to Deploy` | `Done` | Deploy phase merges successfully |
| `Fixes needed` | `In Progress` | Development phase addresses feedback |

### Worktree Management

**Worktree Location**: `.worktrees/linear-{issue-id}-{description}/`

**Creating a Worktree** (use helper script):
```bash
# From project root
./scripts/create_worktree.sh <issue-id> <description>
# Example: ./scripts/create_worktree.sh ABC-123 user-authentication

# Or manually:
git worktree add .worktrees/linear-{issue-id}-{description} -b linear-{issue-id}-{description}
cd .worktrees/linear-{issue-id}-{description}
```

**Removing a Worktree** (after PR merge, use helper script):
```bash
# From project root
./scripts/remove_worktree.sh <issue-id> <description>
# Example: ./scripts/remove_worktree.sh ABC-123 user-authentication

# Or manually:
git worktree remove .worktrees/linear-{issue-id}-{description}
# If worktree directory was deleted manually:
git worktree prune
```

**Listing Worktrees**:
```bash
./scripts/list_worktrees.sh
# Or: git worktree list
```

**Working in a Worktree**: All git operations (commit, push, etc.) happen in the worktree directory. Each worktree has its own `.git` reference pointing to the main repository. Changes are isolated until pushed. Worktrees are stored in `.worktrees/` (in `.gitignore`).

### Staging Slot Management

**Available Slots**: 1-9 for QA; slot 10 temporarily reserved for production.

**Registry**: `project_documentation/staging-slots.md`

**Claiming**: Read registry, find first available slot (1-9), update registry with Status=In Use, Owner, Branch, Issue ID, Deployed At, Purpose. Deploy and test.

**Releasing**: Only when PR merged to main (success) or QA fails. Do NOT release when QA passes but PR not yet merged or when CI fails after merge. Update registry to Available.

### Agent Workflow Core Principles

1. **One Task = One Worktree = One Branch = One Staging Slot** — No sharing between tasks.
2. **Resource Lifecycle** — Worktree created when development starts, removed after PR merge; branch deleted after merge; staging slot claimed at QA start, released when PR merged to main.
3. **Clean Handoffs** — Each phase completes fully before handoff; status changes (via Linear MCP) signal handoff; comments document what was done and what's next.

---

## Project Context
You are working on a **job postings data platform** that ingests, models, enriches, and ranks job postings.

- **Primary Role of the Developer**: Junior data engineer / analytics engineer (explanations should be teaching-oriented and pragmatic).
- **Core Language & Runtime**: Python (3.11+ assumed).
- **Data Stack**:
  - **Database / Warehouse**: PostgreSQL using a **Medallion architecture** (`raw`, `staging`, `marts` schemas).
  - **Transformations**: dbt project.
  - **Orchestration**: Apache Airflow DAG for daily batch execution.
  - **Extraction Services**: Python services for JSearch job postings, Glassdoor company data, and future sources.
  - **Enrichment / Ranking Services**: Python services for skills/seniority enrichment (spaCy + rules) and ranking logic.
  - **Containerization**: Docker / Docker Compose for local development; later AWS (ECS, RDS, S3, EC2).
  - **Testing & Quality**: `pytest` for unit/integration tests; `ruff` for linting + basic formatting.
- **Downstream Consumers**:
  - Daily **email digests** of ranked jobs per profile.
  - **Tableau** dashboards connected to the `marts` schema.
  - Lightweight **profile management interface** (e.g., small Flask app) backed by `marts.profile_preferences`.

- **Key Reference Documents (read and respect)**:
  - Product requirements: `Project Documentation/job-postings-prd.md`
  - API usage examples: `Project Documentation/jsearch.md`, `Project Documentation/glassdoor_companies.md`
  - Naming standards: `Project Documentation/naming_conventions.md`

When in doubt about requirements or naming, prefer the PRD and naming conventions over improvisation.

---

## Core Principles

1. **Explain as you code**
   - When generating or refactoring code or SQL/dbt models, briefly explain:
     - What the change does.
     - How not trivial syntax work. 
     - Why it matches specific design.
     - Any trade-offs or assumptions.

2. **Confirm before major changes**
   - Ask for confirmation **before**:
     - Renaming tables, schemas, or core entities.
     - Changing dbt model contracts.
     - Modifying Airflow DAG structure.
     - Deleting or deprecating code, models, or jobs.

3. **Respect system boundaries**
   - Keep clear separation between:
     - **Extraction** (Python services → `raw`).
     - **Transformation** (dbt → `staging` + `marts`).
     - **Orchestration** (Airflow DAGs).
     - **Consumption** (email digests, Tableau, frontend).
   - Do not push heavy transformation logic into Airflow or the profile UI; keep it in dbt or dedicated services.

4. **Learning-oriented behavior**
   - Assume the developer is learning data engineering:
     - Prefer clear, readable examples over clever one-liners.
     - Show how to test and validate changes (pytest, dbt tests, manual SQL checks).
     - When you introduce a new pattern (e.g., dbt macro, Airflow operator), explain how it fits the overall design.

5. **Clarify ambiguous requirements**
   - When a task is unclear, documentation is incomplete, or there are multiple valid approaches:
     - **Stop and ask** the user to clarify the preferred approach before proceeding.
     - Present the available options with pros/cons if applicable.
     - Do not assume or guess when the requirements are ambiguous.

6. **Human-like codebase**
   - The codebase should appear as if it was written entirely by a human developer.
   - **Never** include comments, docstrings, commit messages, or documentation that mention:
     - AI, AI assistants, or AI tools (e.g., "Generated by AI", "AI-suggested", "Created with ChatGPT").
     - Automated code generation tools.
     - Any references to AI assistance in the development process.
   - Write code, comments, and documentation in a natural, professional developer voice.
   - Keep all the documentation short. 

7. **Staging-first fixing**
   - Always apply fixes and changes to the branch/worktree associated with the staging slot.
   - Avoid making changes in the local environment that aren't immediately pushed to the corresponding staging branch.

8. **No direct push to main**
   - (MANDATORY) Never push code directly to the `main` branch.
   - All work must happen in dedicated feature branches.
   - Merging to `main` is permitted ONLY through approved Pull Requests.

---

## User Request Handling

### When User Requests Implementation

When the user requests to implement something, follow this workflow:

1. **Review necessary files**
   - Read relevant code, documentation, and configuration files.
   - Understand the current implementation and architecture.
   - Identify dependencies and related components.

2. **Clarify uncertainties**
   - Follow [Core Principle #5](#core-principles): If requirements are unclear, documentation is incomplete, or multiple approaches exist, stop and ask the user to clarify before proceeding.

3. **Explain steps briefly**
   - Follow [Core Principle #1](#core-principles): Provide a short explanation of each major step as you implement, explaining what the change does and why it matches the design.

4. **Verify design alignment**
   - Confirm the implementation follows:
     - Medallion architecture (raw → staging → marts).
     - Service boundaries (extractor, enricher, ranker).
     - Naming conventions and project structure.
     - Patterns defined in the PRD.

5. **Verify implementation**
   - After implementation, verify changes by:
     - Running unit tests (`pytest`).
     - Running integration tests where applicable.
     - Running linting (`ruff`).
     - Running dbt compilation and tests for SQL/dbt changes.
     - Manual verification if needed.

6. **Conduct thorough code review**
   - Review the code against the [Code Review Checklist](#code-review-checklist).
   - Check for:
     - Correctness and alignment with requirements.
     - Code quality (type hints, docstrings, naming).
     - Test coverage.
     - Operational concerns (error handling, logging, secrets).

7. **Offer adjustments**
   - Suggest what should be adjusted or fixed.
   - Highlight any potential issues or improvements.
   - Recommend refactoring if code quality can be improved.

8. **Proceed with deployment**
   - Only after all verification and review steps are complete.
   - Ensure the implementation is ready for deployment.

9. **Check CI status after push**
   - **MANDATORY**: After any `git push` to remote repository, automatically check CI status.
   - See [CI Status Checking After Push](#ci-status-checking-after-push) section for detailed workflow.
   - This step is NOT optional - always perform CI status check after pushing code.

### When User Asks a Question

When the user asks a question about code or concepts, explain it briefly and clearly as if teaching a junior developer:

- **What the code does overall**: Provide a high-level summary of the code's purpose and functionality.
- **What each main part/block is responsible for**: Break down the code into logical sections and explain each part's role.
- **Key concepts or terms**: Define important concepts, patterns, or terminology they should know.
- **Avoid unnecessary details**: Focus on what's essential for understanding, skip advanced theory unless relevant.
- **Explain advanced syntax**: If complex Python features, SQL patterns, or dbt macros are used, explain how they work.
- **Keep it short and simple**: Use clear, concise language. Aim for understanding over completeness.

---

## Code Quality Standards

### Python Style & Structure

- **Style Guide**:
  - Follow **PEP 8** with a **maximum line length of 100 characters**.
  - Use **4 spaces** for indentation (no tabs).
  - Organize imports:
    1. Standard library.
    2. Third-party packages.
    3. Local modules.
  - Remove unused imports and variables.

- **Naming** (align with `Project Documentation/naming_conventions.md`):
  - **Python code**:
    - Modules and packages: `snake_case.py` (e.g., `jsearch_client.py`, `jobs_repository.py`).
    - Functions/methods: `snake_case` (e.g., `fetch_jobs_for_profile`, `calculate_rank_score`).
    - Classes: `PascalCase` (e.g., `JSearchClient`, `RankerService`, `ProfileRepository`).
    - Constants: `UPPER_SNAKE_CASE` (e.g., `DEFAULT_PAGE_SIZE`, `JSEARCH_BASE_URL`).
    - Private helpers: prefix with `_` (e.g., `_normalize_employment_types`).
  - **dbt models**:
    - Raw/Staging: `<sourcesystem>_<entity>.sql` (e.g., `jsearch_job_postings.sql`, `glassdoor_companies.sql`).
    - Marts Facts/Dimensions: `<category>_<entity>.sql` (e.g., `fact_jobs.sql`, `dim_companies.sql`, `dim_ranking.sql`).
  - **Airflow DAGs**: `snake_case.py` with descriptive names (e.g., `jobs_etl_daily.py`).
  - **Tests**:
    - Unit: `test_[module].py` (e.g., `test_ranker_service.py`).
    - Integration: `test_[area]_[behavior].py` (e.g., `test_jobs_etl_end_to_end.py`).
  - **Database tables** (see dbt model naming above):
    - Raw/Staging: `<sourcesystem>_<entity>` (e.g., `jsearch_job_postings`, `glassdoor_companies`).
    - Marts: `<category>_<entity>` (e.g., `fact_jobs`, `dim_companies`, `dim_ranking`).
    - Surrogate keys: `<table_name>_key` (e.g., `job_posting_key`, `company_key`).
    - Technical columns: `dwh_<column_name>` (e.g., `dwh_load_date`, `dwh_load_timestamp`).

### Python Type Hints (Required)

- All public functions, methods, and core internal helpers **must** include type hints for parameters and return values.
- Use standard typing imports (`List`, `Dict`, `Optional`, `Literal`, `TypedDict`, `Protocol`, etc.) where helpful.
- Example:

```python
from typing import Dict, List, Optional

class JSearchClient:
    def fetch_jobs(
        self,
        profile_id: int,
        query_params: Dict[str, str],
    ) -> List[Dict]:
        """
        Call JSearch API for a given profile and return a list of job posting payloads.
        """
        ...
```

### Docstrings (Required)

- Provide docstrings for:
  - Public classes.
  - Public functions/methods.
  - Any function that implements a non-trivial business rule (e.g., ranking logic).

- Use Google-style.

```python
def calculate_rank_score(
    job: Dict[str, str],
    profile: Dict[str, str],
) -> float:
    """
    Compute a relevance score for a single job/profile pair.

    Args:
        job: Normalized job posting fields from marts.fact_jobs.
        profile: Profile preferences from marts.profile_preferences.

    Returns:
        A score in the range [0.0, 100.0].
    """
    ...
```

### dbt SQL & Model Standards

- **Formatting**:
  - Use lowercase SQL keywords where practical, and align joins/conditions for readability.
  - One column per line in `SELECT` clauses for core models.
  - Prefer CTEs for complex logic, each with a clear, descriptive name.

- **Model Types**:
  - `raw` and `staging` models: typically **views** or **incremental** where appropriate.
  - `marts` models: prefer **tables** or **incremental** materializations.

- **Tests**: See [dbt Tests](#dbt-tests) section for detailed testing requirements.

### Linting & Formatting

- **Tooling**:
  - Use `ruff` as the primary linter.
  - Ensure no:
    - Unused imports.
    - Unused variables.
    - Bare `except:` clauses.
    - Debugging artifacts (`print`, stray logging, `pdb.set_trace()`).

- Before committing:
  - Run `ruff` on Python packages/services.
  - Run `dbt compile` (and `dbt test` if applicable) for SQL/dbt changes.

---

## Project Structure

> Note: This project is under active development; these guidelines describe the **intended** final structure based on the PRD. When implementing new files, follow this structure even if all folders do not yet exist.

### High-Level Organization (Example Target Layout)

```text
.
├── Project Documentation/          # PRDs, API docs, diagrams, naming conventions
├── services/
│   ├── extractor/                  # Python extraction service(s) for JSearch, Glassdoor, etc.
│   ├── enricher/                   # Python enrichment service(s) for skills/seniority
│   └── ranker/                     # Python ranking service(s)
├── airflow/
│   ├── dags/                       # Airflow DAGs (e.g., jobs_etl_daily)
│   ├── plugins/                    # Custom operators/sensors if needed
│   └── config/                     # Connections, environment configs (non-secret)
├── dbt/
│   ├── models/
│   │   ├── raw/                    # raw.* models
│   │   ├── staging/                # staging.* models
│   │   └── marts/                  # marts.* models (fact/dim)
│   ├── tests/                      # dbt schema + data tests
│   └── macros/                     # Common SQL macros
├── ui/
│   └── profile_manager/            # Lightweight profile management UI (Flask or similar)
├── infra/
│   ├── docker/                     # Dockerfiles, docker-compose definitions
│   ├── sql/                        # Schema and table initialization scripts (tables created before services run)
│   └── ci-cd/                      # CI/CD scripts and configs
├── tests/
│   ├── unit/                       # Python unit tests
│   └── integration/                # Integration tests (db, services, Airflow)
└── .cursor/
    └── rules/                      # Cursor configuration
```

---

## Architecture Patterns

### Medallion Architecture (Raw → Staging → Marts)

- **Raw (`raw` schema / Bronze)**:
  - Stores near-original JSON payloads from JSearch, Glassdoor, and future APIs.
  - Minimal transformation: mainly unpacking API responses into rows and capturing metadata.
  - Tables are created automatically by Docker initialization scripts before services run (see [Orchestration with Airflow](#orchestration-with-airflow)).

- **Staging (`staging` schema / Silver)**:
  - Normalized, cleaned, and de-duplicated relational data.
  - Columns typed appropriately (dates, numeric fields, booleans, enums).
  - Includes technical columns with `dwh_` prefix (see [Naming conventions](#code-quality-standards)).

- **Marts (`marts` schema / Gold)**:
  - Business-ready **fact** and **dimension** tables plus configuration tables:
    - `fact_jobs`
    - `dim_companies`
    - `dim_ranking`
    - `profile_preferences`

### Service Boundaries

- **Extractor Service(s)**:
  - Responsibilities:
    - Handle API authentication, rate limiting, retries, and logging.
    - Map profile preferences to API calls.
    - Write raw payloads into `raw.*` tables with appropriate technical metadata.
  - Must **not** contain downstream transformation or ranking logic.

- **Enricher Service(s)**:
  - Responsibilities:
    - Skill extraction (e.g., via spaCy models).
    - Seniority extraction using well-documented rules.
    - Write enrichment fields back into staging (e.g., `staging.jsearch_job_postings`).

- **Ranker Service(s)**:
  - Responsibilities:
    - Implement ranking algorithms per the PRD (MVP then extended).
    - Read from `marts.fact_jobs` and `marts.profile_preferences`.
    - Write to `marts.dim_ranking` with `rank_score` and later `rank_explain`.
  - Should be deterministic and testable via unit tests and small fixtures.

### Orchestration with Airflow

- DAG `jobs_etl_daily` should:
  - Follow the task list and order from the PRD (extract → normalize → enrich → model → rank → test → notify).
  - Use clearly named tasks reflecting each stage (e.g., `extract_job_postings`, `normalize_jobs`, `rank_jobs`).
  - Use IDs, not names, in dependencies (`task1 >> task2`).
  - Have retry and timeout policies appropriate to each task (e.g., longer for external APIs).
  - Note: Tables are created automatically by Docker initialization scripts (`docker/init/02_create_tables.sql`) before DAG execution, so no initialization task is needed.

---

## Testing Standards

### Python Tests (pytest)

- **Unit Tests**:
  - Cover:
    - API client parameter construction and error handling.
    - Core transformation helpers (e.g., normalizing JSON fields before dbt).
    - Ranking logic (e.g., scores for known job/profile combinations).
  - Use small, focused fixtures and avoid external dependencies.

- **Integration Tests**:
  - Validate end-to-end flows in a constrained scope:
    - Extractor → raw tables (using a test DB).
    - dbt models from raw to staging to marts.
    - Ranker service reading from and writing to marts.
  - Mark appropriately (e.g., `@pytest.mark.integration`) so they can be included/excluded in CI.

### dbt Tests

- For **every core model** (especially in `marts`):
  - Define `unique` and `not_null` tests on primary keys (e.g., `job_posting_key`).
  - Define `relationships` tests between facts and dimensions.
  - Add business rule tests where feasible (e.g., `rank_score between 0 and 100`).

### Airflow DAG & Pipeline Tests

- Keep pipeline tests pragmatic:
  - At minimum, have tests that:
    - Import DAG files without errors.
    - Assert DAG has expected tasks and dependencies.
  - Optionally, use Airflow test facilities or custom harnesses for critical operators.

### Browser Tests (Playwright)

- **Purpose**: End-to-end testing of UI functionality using headless browser automation.
- **Location**: `tests/browser/` directory
- **Framework**: Playwright for Python
- **When to use**: 
  - Testing user workflows (login, navigation, form submission)
  - Verifying UI changes affect behavior correctly
  - Testing database-related features through the UI
  - QA verification before merging PRs

- **Running Browser Tests**:
  - **Prerequisites**: 
    - Flask server must be running (configured via `FLASK_APP_URL` environment variable, default: `http://localhost:5000`)
    - External database configured in environment variables (POSTGRES_HOST, POSTGRES_PORT, etc.)
    - Frontend must be built (located in `frontend/dist/`)
  - **Command**: `pytest tests/browser/ -v`
  - **For cloud agents**: Flask server should start automatically via terminals in `.cursor/environment.json`, or agents should start it manually before running tests

- **Test Structure**:
  - Use fixtures from `tests/browser/conftest.py`:
    - `page`: Basic Playwright page fixture (waits for Flask server)
    - `logged_in_page`: Page with authenticated session
    - `app_url`: Flask app URL from environment
  - Example test structure:
    ```python
    def test_my_feature(logged_in_page: Page, app_url: str) -> None:
        """Test my feature."""
        page = logged_in_page
        page.goto(f"{app_url}/my-feature")
        # Interact with elements and assert expected behavior
    ```

- **Taking Screenshots**:
  - Use `page.screenshot()` to capture screenshots during tests
  - Save to `tests/browser/screenshots/` directory with descriptive names
  - Example:
    ```python
    page.screenshot(path="tests/browser/screenshots/dashboard_after_login.png")
    ```
  - **Screenshot naming**: Use descriptive names like `feature_name_state.png` (e.g., `campaigns_page_loaded.png`, `job_details_with_notes.png`)

- **For Cloud Agents**:
  - Install Playwright: `pip install playwright && python -m playwright install chromium`
  - Build frontend: `cd frontend && npm ci && npm run build && cd ..`
  - Start Flask server: `cd campaign_ui && python app.py &` (in background)
  - Run tests: `pytest tests/browser/ -v`
  - See `tests/browser/README.md` for detailed setup instructions

- **Attaching Screenshots to Linear Issues**:
  - **When testing in QA phase**:
    1. Take screenshots using `page.screenshot()` during browser tests
    2. Save screenshots to `tests/browser/screenshots/` directory
    3. Reference screenshots in Linear issue comments using markdown:
       ```markdown
       Browser test screenshots:
       - Dashboard: `tests/browser/screenshots/dashboard_after_login.png`
       - Campaigns page: `tests/browser/screenshots/campaigns_page_loaded.png`
       ```
    4. If screenshots need to be accessible in Linear UI:
       - Option A: Commit screenshots to repository and reference in comments
       - Option B: Upload screenshots to a public location and include image links in markdown
       - Option C: Describe screenshots in comments if file attachment is not directly supported
  - **Screenshot best practices**:
    - Take screenshots at key verification points (after login, after form submission, after state changes)
    - Include screenshots in Linear comments when reporting test results
    - Name screenshots descriptively to indicate what they verify

---

## Version Control Strategy

### GitHub Operations via MCP

**IMPORTANT**: When performing GitHub-specific operations, **always use MCP (Model Context Protocol) GitHub tools** instead of direct git commands or manual GitHub UI interactions.

**MANDATORY**: Direct pushes to `main` are strictly forbidden. All merges to `main` must occur via Pull Requests managed through MCP.

- **Use MCP GitHub tools for**:
  - Creating, updating, and managing issues
  - Creating, updating, reviewing, and merging pull requests
  - Creating and managing branches
  - Searching code, repositories, issues, and pull requests
  - Managing releases and tags
  - Getting repository information, file contents, and commit details
  - Any GitHub API operations

- **Use standard git commands only for**:
  - Local git operations (commit, add, status, log, etc.)
  - Local branch operations
  - Local file operations

- **When to use MCP GitHub tools**:
  - Creating pull requests: Use `mcp_github_create_pull_request` instead of pushing and creating PR manually
  - Reviewing PRs: Use `mcp_github_pull_request_read` and review tools instead of manual review
  - Managing issues: Use `mcp_github_create_issue`, `mcp_github_update_issue`, etc.
  - Searching codebase: Use `mcp_github_search_code` for cross-repository searches
  - Getting file contents from GitHub: Use `mcp_github_get_file_contents` instead of cloning

- **Best practices**:
  - Always check repository owner and name before making MCP calls
  - Use `mcp_github_get_me` to get authenticated user information when needed
  - For PR reviews, create a pending review first, add comments, then submit
  - Verify branch names and PR numbers before operations
  - Handle errors gracefully and provide clear feedback

### DigitalOcean Operations via MCP

**IMPORTANT**: When performing DigitalOcean-specific operations for environment control and deployment, **always use MCP (Model Context Protocol) DigitalOcean tools** instead of manual console interactions or SSH commands where possible.

- **Use MCP DigitalOcean tools for**:
  - Listing and managing droplets (check status, get information)
  - Managing database clusters (list, get status, check health)
  - Monitoring actions and deployment status
  - Verifying infrastructure health before deployments
  - Environment verification during QA and deployment phases

- **Use deployment scripts for**:
  - Actual code deployment (`deploy-staging.sh`, `deploy-production.sh`)
  - SSH-based operations on droplets
  - Running Docker Compose commands

- **When to use MCP DigitalOcean tools**:
  - Before deployment: Check droplet status and database cluster health
  - During QA: Verify environment is ready for testing
  - After deployment: Confirm services are running correctly
  - Troubleshooting: Get droplet/database information for debugging
  - Production deployment: Verify production environment (slot 10) status

- **Best practices**:
  - Use `droplet-list` and `droplet-get` to verify droplet status before deployments
  - Use `db-cluster-list` and `db-cluster-get` to check database cluster health
  - Use `action-list` and `action-get` to monitor deployment actions
  - Combine MCP tools with deployment scripts for complete deployment workflow
  - Always verify infrastructure status before and after deployments

### Branch Naming

**For Linear-tracked work**: See [Agent Workflow](#agent-workflow) section above. Use `linear-{issue-id}-{short-description}` format.

**For non-Linear work** (experiments, personal branches):
- Format: `[type]/[short-description]`
- Types:
  - `feature/` – New features or major components (e.g., `feature/add-ranker-service`).
  - `bugfix/` – Bug fixes (e.g., `bugfix/fix-dbt-key-violation`).
  - `chore/` – Maintenance, tooling, or config (e.g., `chore/update-ruff-config`).
  - `docs/` – Documentation changes (e.g., `docs/update-job-postings-prd`).
  - `experiment/` – Throwaway experiments or spikes (should not merge directly).

### Commit Messages

- Use a simple, consistent structure:

```text
[TYPE]: Brief description (max 50 chars)

Optional longer description wrapped at ~72 chars.

- Bullet points for key changes
- Reference tickets/issues where relevant (e.g., #123)
```

- Example:

```text
feature: Add initial jobs extractor service

- Implement JSearch API client with basic params
- Write responses into raw.jsearch_job_postings
- Add unit tests for query construction
```


### Commit Hygiene

- Keep commits **atomic**:
  - One logical change per commit (e.g., do not mix dbt refactors and Airflow DAG restructuring).
- Ensure:
  - Python tests pass (or are updated) for affected areas.
  - dbt models at least compile for schema-altering changes.
  - Linting (ruff) is clean for changed files.

### CI Status Checking After Push

**MANDATORY REQUIREMENT**: After EVERY push to the remote repository (`git push`), the agent MUST automatically check CI status and report results to the user. This is a required step that must be performed without user prompting.

**Enforcement**: This check MUST happen immediately after any successful `git push` command completes. Do not wait for user instruction - perform the check automatically.

**CRITICAL**: The agent MUST continue polling CI status until the workflow run reaches a terminal state ("completed", "failure", "cancelled", or "skipped"). Do NOT stop polling after a fixed timeout. Keep checking until completion.

#### Workflow

1. **Wait for CI initiation**: After push completes, wait 10-15 seconds for GitHub Actions to start the workflow run, then begin polling.

2. **Poll for CI completion** (MANDATORY - continue until completion): 
   - **Poll continuously** every 15-30 seconds
   - **DO NOT stop polling** until the workflow status is a terminal state:
     - `completed` (success or failure)
     - `failure`
     - `cancelled`
     - `skipped`
   - Check status using `.github/scripts/query_ci_errors.py`:
     ```bash
     python .github/scripts/query_ci_errors.py \
       --repo <owner>/<repo> \
       --token $GITHUB_TOKEN \
       --latest \
       --branch <branch_name> \
       --jobs \
       --output-json
     ```
   - **Continue polling indefinitely** until terminal state is reached
   - If workflow is still `in_progress` or `queued`, wait and check again
   - Only stop when status is `completed`, `failure`, `cancelled`, or `skipped`

3. **Report CI status to user**:
   
   **If CI passed**:
   - Inform user: "✅ CI checks passed successfully for branch <branch_name>"
   - Show summary of jobs that ran (lint-and-format, test, dbt-test)
   
   **If CI failed**:
   - Extract detailed error information using `.github/scripts/report_ci_errors.py`:
     ```bash
     python .github/scripts/report_ci_errors.py \
       --workflow-run-id <run_id> \
       --repo <owner>/<repo> \
       --token $GITHUB_TOKEN \
       --output-json
     ```
   - Present a clear, structured error report:
     - **Summary**: List failed jobs and total error count
     - **For each failed job**, show:
       - **Linting errors**: File path, line:column, error code, and message
       - **Test failures**: Test name and error details (truncate long messages)
       - **dbt test failures**: Test name, location, and error details
     - Include link to the workflow run in GitHub Actions UI
   - **Offer to fix**: Ask user if they want help fixing the errors

4. **Error handling**:
   - If GitHub API access fails: "❌ Unable to check CI status. Ensure GITHUB_TOKEN is set with repo and actions:read permissions."
   - If workflow is stuck in `in_progress` or `queued` for an unusually long time (>30 minutes), inform user: "⚠️ CI workflow is taking longer than expected. Current status: <status>. Continue monitoring or check GitHub Actions UI manually: <url>"
   - **Note**: Do not give up on polling - continue checking until a terminal state is reached

#### Implementation Notes

- **Continuous Polling**: The agent MUST implement a polling loop that continues until CI reaches a terminal state. Do not exit the polling loop early.
- Use `github.repository` and current branch name from git context
- Store workflow run ID from initial status check for error extraction
- Parse JSON output from scripts programmatically
- Format error output in a readable, structured manner for the user
- If user requests fixes, analyze errors and suggest code changes
- **Polling Strategy**:
  - Start polling 10-15 seconds after push completes
  - Poll every 15-30 seconds
  - Check workflow status on each poll
  - Continue until status is `completed`, `failure`, `cancelled`, or `skipped`
  - Log polling progress to user (e.g., "Waiting for CI to complete... (status: in_progress)")
- **Token Access**: 
  - Preferred: Use `GITHUB_TOKEN` environment variable if available
  - Alternative: Use MCP GitHub tools if available (mcp_github_list_commits, etc.)
  - Fallback: Prompt user if token is not available
- **Dependencies**: Scripts require `PyGithub` and `requests` packages. Install with `pip install PyGithub requests` if running locally.

---

## Code Review Checklist

Before considering code "done", check:

- **Correctness & Requirements**
  - [ ] Implementation aligns with `Project Documentation/job-postings-prd.md`.
  - [ ] Data structures (schemas, tables, columns) follow naming conventions.
  - [ ] No behavior conflicts with the Medallion architecture or service boundaries.

- **Python Code Quality**
  - [ ] Functions and classes have appropriate type hints.
  - [ ] Public functions/methods have meaningful docstrings.
  - [ ] No unused imports or variables.
  - [ ] Logging is used instead of `print`, with appropriate log levels.
  - [ ] Code follows bets practicel like DRY code and Separation Of Concerns.

- **dbt Models & SQL**
  - [ ] Model names and schema locations are correct (raw/staging/marts).
  - [ ] CTEs are named clearly; complex logic is broken into manageable pieces.
  - [ ] dbt schema files updated with relevant tests for new/changed models.

- **Testing**
  - [ ] New logic is covered by unit tests where feasible.
  - [ ] Relevant integration or pipeline tests updated or added.
  - [ ] `dbt test` succeeds for impacted models (where applicable).

- **Operational Concerns**
  - [ ] Airflow DAG changes preserve task order and idempotency where needed.
  - [ ] External API usage (JSearch, Glassdoor) respects rate limits and handles errors.
  - [ ] Secrets are not hard-coded (use env vars).

- **Documentation**
  - [ ] Documentation updated where needed (PRD sections, design notes, or inline comments).

---

## Reference Documents

When making design or implementation decisions, consult:

- **Product Requirements**:
  - `Project Documentation/job-postings-prd.md`
- **API Usage and Payloads**:
  - `Project Documentation/jsearch.md`
  - `Project Documentation/glassdoor_companies.md` (if present)
- **Data Modeling & Naming**:
  - `Project Documentation/naming_conventions.md`
  - Any data modeling diagrams in `Project Documentation/` (e.g., data modeling and architecture diagrams).
- **Implementation Planning**:
  - `Project Documentation/implementation-todo.md` (if present) or similar planning docs.

If new patterns, tables, or services are introduced, they should be:

1. Consistent with these rules.
2. Reflected in the PRD and/or supporting documentation.
3. Explained in code comments/docstrings where non-obvious.

---