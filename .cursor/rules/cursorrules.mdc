---
alwaysApply: true
---


## Sections in This File
1. [**MANDATORY: Linear Task Workflow**](#mandatory-linear-task-workflow) ← READ FIRST FOR LINEAR TASKS
2. [**Project Context**](#project-context)
3. [**Core Principles**](#core-principles)
4. [**User Request Handling**](#user-request-handling)
5. [**Code Quality Standards**](#code-quality-standards)
6. [**Project Structure**](#project-structure)
7. [**Architecture Patterns**](#architecture-patterns)
8. [**Testing Standards**](#testing-standards)
9. [**Version Control Strategy**](#version-control-strategy)
   - [**GitHub Operations via MCP**](#github-operations-via-mcp)
10. [**Code Review Checklist**](#code-review-checklist)
11. [**Reference Documents**](#reference-documents)
12. [**Linear Workflow**](#linear-workflow)
13. [**Agent Workflow**](#agent-workflow)

## MANDATORY: Linear Task Workflow

**BEFORE starting ANY work on a Linear issue, you MUST:**

1. **Read the agent workflow**: `project_documentation/agent-workflow.md` - Contains complete workflow for all agent phases
2. **Understand status-based triggers**: Agents automatically pick up tasks based on Linear issue status (see [Agent Workflow](#agent-workflow) section)
3. **Use correct branch naming**: `linear-{issue-id}-{short-description}` (e.g., `linear-JOB-36-fix-duplicates`)
4. **Follow isolation rules**: Each issue gets its own worktree, branch, and staging slot (if relevant)

**CRITICAL RULES**:
- **Branch naming**: MUST use `linear-{issue-id}-{short-description}` format (NOT `cursor/...`, `feature/...`, etc.)
- **Worktrees**: Each Linear issue gets its own worktree in `.worktrees/linear-{issue-id}-{description}/`
- **Status updates**: Update Linear status via MCP at each phase - status changes automatically trigger next agent
- **MCP tools**: Use Linear MCP for status/comments, GitHub MCP for PRs, DigitalOcean MCP for environment control and deployment

**For complete details**, see:
- `project_documentation/agent-workflow.md` - Detailed workflow steps for each agent phase

---

## Project Context
You are working on a **job postings data platform** that ingests, models, enriches, and ranks job postings.

- **Primary Role of the Developer**: Junior data engineer / analytics engineer (explanations should be teaching-oriented and pragmatic).
- **Core Language & Runtime**: Python (3.11+ assumed).
- **Data Stack**:
  - **Database / Warehouse**: PostgreSQL using a **Medallion architecture** (`raw`, `staging`, `marts` schemas).
  - **Transformations**: dbt project.
  - **Orchestration**: Apache Airflow DAG for daily batch execution.
  - **Extraction Services**: Python services for JSearch job postings, Glassdoor company data, and future sources.
  - **Enrichment / Ranking Services**: Python services for skills/seniority enrichment (spaCy + rules) and ranking logic.
  - **Containerization**: Docker / Docker Compose for local development; later AWS (ECS, RDS, S3, EC2).
  - **Testing & Quality**: `pytest` for unit/integration tests; `ruff` for linting + basic formatting.
- **Downstream Consumers**:
  - Daily **email digests** of ranked jobs per profile.
  - **Tableau** dashboards connected to the `marts` schema.
  - Lightweight **profile management interface** (e.g., small Flask app) backed by `marts.profile_preferences`.

- **Key Reference Documents (read and respect)**:
  - Product requirements: `Project Documentation/job-postings-prd.md`
  - API usage examples: `Project Documentation/jsearch.md`, `Project Documentation/glassdoor_companies.md`
  - Naming standards: `Project Documentation/naming_conventions.md`

When in doubt about requirements or naming, prefer the PRD and naming conventions over improvisation.

---

## Core Principles

1. **Explain as you code**
   - When generating or refactoring code or SQL/dbt models, briefly explain:
     - What the change does.
     - How not trivial syntax work. 
     - Why it matches specific design.
     - Any trade-offs or assumptions.

2. **Confirm before major changes**
   - Ask for confirmation **before**:
     - Renaming tables, schemas, or core entities.
     - Changing dbt model contracts.
     - Modifying Airflow DAG structure.
     - Deleting or deprecating code, models, or jobs.

3. **Respect system boundaries**
   - Keep clear separation between:
     - **Extraction** (Python services → `raw`).
     - **Transformation** (dbt → `staging` + `marts`).
     - **Orchestration** (Airflow DAGs).
     - **Consumption** (email digests, Tableau, frontend).
   - Do not push heavy transformation logic into Airflow or the profile UI; keep it in dbt or dedicated services.

4. **Learning-oriented behavior**
   - Assume the developer is learning data engineering:
     - Prefer clear, readable examples over clever one-liners.
     - Show how to test and validate changes (pytest, dbt tests, manual SQL checks).
     - When you introduce a new pattern (e.g., dbt macro, Airflow operator), explain how it fits the overall design.

5. **Clarify ambiguous requirements**
   - When a task is unclear, documentation is incomplete, or there are multiple valid approaches:
     - **Stop and ask** the user to clarify the preferred approach before proceeding.
     - Present the available options with pros/cons if applicable.
     - Do not assume or guess when the requirements are ambiguous.

6. **Human-like codebase**
   - The codebase should appear as if it was written entirely by a human developer.
   - **Never** include comments, docstrings, commit messages, or documentation that mention:
     - AI, AI assistants, or AI tools (e.g., "Generated by AI", "AI-suggested", "Created with ChatGPT").
     - Automated code generation tools.
     - Any references to AI assistance in the development process.
   - Write code, comments, and documentation in a natural, professional developer voice.
   - Keep all the documentation short. 

---

## User Request Handling

### When User Requests Implementation

When the user requests to implement something, follow this workflow:

1. **Review necessary files**
   - Read relevant code, documentation, and configuration files.
   - Understand the current implementation and architecture.
   - Identify dependencies and related components.

2. **Clarify uncertainties**
   - Follow [Core Principle #5](#core-principles): If requirements are unclear, documentation is incomplete, or multiple approaches exist, stop and ask the user to clarify before proceeding.

3. **Explain steps briefly**
   - Follow [Core Principle #1](#core-principles): Provide a short explanation of each major step as you implement, explaining what the change does and why it matches the design.

4. **Verify design alignment**
   - Confirm the implementation follows:
     - Medallion architecture (raw → staging → marts).
     - Service boundaries (extractor, enricher, ranker).
     - Naming conventions and project structure.
     - Patterns defined in the PRD.

5. **Verify implementation**
   - After implementation, verify changes by:
     - Running unit tests (`pytest`).
     - Running integration tests where applicable.
     - Running linting (`ruff`).
     - Running dbt compilation and tests for SQL/dbt changes.
     - Manual verification if needed.

6. **Conduct thorough code review**
   - Review the code against the [Code Review Checklist](#code-review-checklist).
   - Check for:
     - Correctness and alignment with requirements.
     - Code quality (type hints, docstrings, naming).
     - Test coverage.
     - Operational concerns (error handling, logging, secrets).

7. **Offer adjustments**
   - Suggest what should be adjusted or fixed.
   - Highlight any potential issues or improvements.
   - Recommend refactoring if code quality can be improved.

8. **Proceed with deployment**
   - Only after all verification and review steps are complete.
   - Ensure the implementation is ready for deployment.

9. **Check CI status after push**
   - **MANDATORY**: After any `git push` to remote repository, automatically check CI status.
   - See [CI Status Checking After Push](#ci-status-checking-after-push) section for detailed workflow.
   - This step is NOT optional - always perform CI status check after pushing code.

### When User Asks a Question

When the user asks a question about code or concepts, explain it briefly and clearly as if teaching a junior developer:

- **What the code does overall**: Provide a high-level summary of the code's purpose and functionality.
- **What each main part/block is responsible for**: Break down the code into logical sections and explain each part's role.
- **Key concepts or terms**: Define important concepts, patterns, or terminology they should know.
- **Avoid unnecessary details**: Focus on what's essential for understanding, skip advanced theory unless relevant.
- **Explain advanced syntax**: If complex Python features, SQL patterns, or dbt macros are used, explain how they work.
- **Keep it short and simple**: Use clear, concise language. Aim for understanding over completeness.

---

## Code Quality Standards

### Python Style & Structure

- **Style Guide**:
  - Follow **PEP 8** with a **maximum line length of 100 characters**.
  - Use **4 spaces** for indentation (no tabs).
  - Organize imports:
    1. Standard library.
    2. Third-party packages.
    3. Local modules.
  - Remove unused imports and variables.

- **Naming** (align with `Project Documentation/naming_conventions.md`):
  - **Python code**:
    - Modules and packages: `snake_case.py` (e.g., `jsearch_client.py`, `jobs_repository.py`).
    - Functions/methods: `snake_case` (e.g., `fetch_jobs_for_profile`, `calculate_rank_score`).
    - Classes: `PascalCase` (e.g., `JSearchClient`, `RankerService`, `ProfileRepository`).
    - Constants: `UPPER_SNAKE_CASE` (e.g., `DEFAULT_PAGE_SIZE`, `JSEARCH_BASE_URL`).
    - Private helpers: prefix with `_` (e.g., `_normalize_employment_types`).
  - **dbt models**:
    - Raw/Staging: `<sourcesystem>_<entity>.sql` (e.g., `jsearch_job_postings.sql`, `glassdoor_companies.sql`).
    - Marts Facts/Dimensions: `<category>_<entity>.sql` (e.g., `fact_jobs.sql`, `dim_companies.sql`, `dim_ranking.sql`).
  - **Airflow DAGs**: `snake_case.py` with descriptive names (e.g., `jobs_etl_daily.py`).
  - **Tests**:
    - Unit: `test_[module].py` (e.g., `test_ranker_service.py`).
    - Integration: `test_[area]_[behavior].py` (e.g., `test_jobs_etl_end_to_end.py`).
  - **Database tables** (see dbt model naming above):
    - Raw/Staging: `<sourcesystem>_<entity>` (e.g., `jsearch_job_postings`, `glassdoor_companies`).
    - Marts: `<category>_<entity>` (e.g., `fact_jobs`, `dim_companies`, `dim_ranking`).
    - Surrogate keys: `<table_name>_key` (e.g., `job_posting_key`, `company_key`).
    - Technical columns: `dwh_<column_name>` (e.g., `dwh_load_date`, `dwh_load_timestamp`).

### Python Type Hints (Required)

- All public functions, methods, and core internal helpers **must** include type hints for parameters and return values.
- Use standard typing imports (`List`, `Dict`, `Optional`, `Literal`, `TypedDict`, `Protocol`, etc.) where helpful.
- Example:

```python
from typing import Dict, List, Optional

class JSearchClient:
    def fetch_jobs(
        self,
        profile_id: int,
        query_params: Dict[str, str],
    ) -> List[Dict]:
        """
        Call JSearch API for a given profile and return a list of job posting payloads.
        """
        ...
```

### Docstrings (Required)

- Provide docstrings for:
  - Public classes.
  - Public functions/methods.
  - Any function that implements a non-trivial business rule (e.g., ranking logic).

- Use Google-style.

```python
def calculate_rank_score(
    job: Dict[str, str],
    profile: Dict[str, str],
) -> float:
    """
    Compute a relevance score for a single job/profile pair.

    Args:
        job: Normalized job posting fields from marts.fact_jobs.
        profile: Profile preferences from marts.profile_preferences.

    Returns:
        A score in the range [0.0, 100.0].
    """
    ...
```

### dbt SQL & Model Standards

- **Formatting**:
  - Use lowercase SQL keywords where practical, and align joins/conditions for readability.
  - One column per line in `SELECT` clauses for core models.
  - Prefer CTEs for complex logic, each with a clear, descriptive name.

- **Model Types**:
  - `raw` and `staging` models: typically **views** or **incremental** where appropriate.
  - `marts` models: prefer **tables** or **incremental** materializations.

- **Tests**: See [dbt Tests](#dbt-tests) section for detailed testing requirements.

### Linting & Formatting

- **Tooling**:
  - Use `ruff` as the primary linter.
  - Ensure no:
    - Unused imports.
    - Unused variables.
    - Bare `except:` clauses.
    - Debugging artifacts (`print`, stray logging, `pdb.set_trace()`).

- Before committing:
  - Run `ruff` on Python packages/services.
  - Run `dbt compile` (and `dbt test` if applicable) for SQL/dbt changes.

---

## Project Structure

> Note: This project is under active development; these guidelines describe the **intended** final structure based on the PRD. When implementing new files, follow this structure even if all folders do not yet exist.

### High-Level Organization (Example Target Layout)

```text
.
├── Project Documentation/          # PRDs, API docs, diagrams, naming conventions
├── services/
│   ├── extractor/                  # Python extraction service(s) for JSearch, Glassdoor, etc.
│   ├── enricher/                   # Python enrichment service(s) for skills/seniority
│   └── ranker/                     # Python ranking service(s)
├── airflow/
│   ├── dags/                       # Airflow DAGs (e.g., jobs_etl_daily)
│   ├── plugins/                    # Custom operators/sensors if needed
│   └── config/                     # Connections, environment configs (non-secret)
├── dbt/
│   ├── models/
│   │   ├── raw/                    # raw.* models
│   │   ├── staging/                # staging.* models
│   │   └── marts/                  # marts.* models (fact/dim)
│   ├── tests/                      # dbt schema + data tests
│   └── macros/                     # Common SQL macros
├── ui/
│   └── profile_manager/            # Lightweight profile management UI (Flask or similar)
├── infra/
│   ├── docker/                     # Dockerfiles, docker-compose definitions
│   ├── sql/                        # Schema and table initialization scripts (tables created before services run)
│   └── ci-cd/                      # CI/CD scripts and configs
├── tests/
│   ├── unit/                       # Python unit tests
│   └── integration/                # Integration tests (db, services, Airflow)
└── .cursor/
    └── rules/                      # Cursor configuration
```

---

## Architecture Patterns

### Medallion Architecture (Raw → Staging → Marts)

- **Raw (`raw` schema / Bronze)**:
  - Stores near-original JSON payloads from JSearch, Glassdoor, and future APIs.
  - Minimal transformation: mainly unpacking API responses into rows and capturing metadata.
  - Tables are created automatically by Docker initialization scripts before services run (see [Orchestration with Airflow](#orchestration-with-airflow)).

- **Staging (`staging` schema / Silver)**:
  - Normalized, cleaned, and de-duplicated relational data.
  - Columns typed appropriately (dates, numeric fields, booleans, enums).
  - Includes technical columns with `dwh_` prefix (see [Naming conventions](#code-quality-standards)).

- **Marts (`marts` schema / Gold)**:
  - Business-ready **fact** and **dimension** tables plus configuration tables:
    - `fact_jobs`
    - `dim_companies`
    - `dim_ranking`
    - `profile_preferences`

### Service Boundaries

- **Extractor Service(s)**:
  - Responsibilities:
    - Handle API authentication, rate limiting, retries, and logging.
    - Map profile preferences to API calls.
    - Write raw payloads into `raw.*` tables with appropriate technical metadata.
  - Must **not** contain downstream transformation or ranking logic.

- **Enricher Service(s)**:
  - Responsibilities:
    - Skill extraction (e.g., via spaCy models).
    - Seniority extraction using well-documented rules.
    - Write enrichment fields back into staging (e.g., `staging.jsearch_job_postings`).

- **Ranker Service(s)**:
  - Responsibilities:
    - Implement ranking algorithms per the PRD (MVP then extended).
    - Read from `marts.fact_jobs` and `marts.profile_preferences`.
    - Write to `marts.dim_ranking` with `rank_score` and later `rank_explain`.
  - Should be deterministic and testable via unit tests and small fixtures.

### Orchestration with Airflow

- DAG `jobs_etl_daily` should:
  - Follow the task list and order from the PRD (extract → normalize → enrich → model → rank → test → notify).
  - Use clearly named tasks reflecting each stage (e.g., `extract_job_postings`, `normalize_jobs`, `rank_jobs`).
  - Use IDs, not names, in dependencies (`task1 >> task2`).
  - Have retry and timeout policies appropriate to each task (e.g., longer for external APIs).
  - Note: Tables are created automatically by Docker initialization scripts (`docker/init/02_create_tables.sql`) before DAG execution, so no initialization task is needed.

---

## Testing Standards

### Python Tests (pytest)

- **Unit Tests**:
  - Cover:
    - API client parameter construction and error handling.
    - Core transformation helpers (e.g., normalizing JSON fields before dbt).
    - Ranking logic (e.g., scores for known job/profile combinations).
  - Use small, focused fixtures and avoid external dependencies.

- **Integration Tests**:
  - Validate end-to-end flows in a constrained scope:
    - Extractor → raw tables (using a test DB).
    - dbt models from raw to staging to marts.
    - Ranker service reading from and writing to marts.
  - Mark appropriately (e.g., `@pytest.mark.integration`) so they can be included/excluded in CI.

### dbt Tests

- For **every core model** (especially in `marts`):
  - Define `unique` and `not_null` tests on primary keys (e.g., `job_posting_key`).
  - Define `relationships` tests between facts and dimensions.
  - Add business rule tests where feasible (e.g., `rank_score between 0 and 100`).

### Airflow DAG & Pipeline Tests

- Keep pipeline tests pragmatic:
  - At minimum, have tests that:
    - Import DAG files without errors.
    - Assert DAG has expected tasks and dependencies.
  - Optionally, use Airflow test facilities or custom harnesses for critical operators.

### Browser Tests (Playwright)

- **Purpose**: End-to-end testing of UI functionality using headless browser automation.
- **Location**: `tests/browser/` directory
- **Framework**: Playwright for Python
- **When to use**: 
  - Testing user workflows (login, navigation, form submission)
  - Verifying UI changes affect behavior correctly
  - Testing database-related features through the UI
  - QA verification before merging PRs

- **Running Browser Tests**:
  - **Prerequisites**: 
    - Flask server must be running (configured via `FLASK_APP_URL` environment variable, default: `http://localhost:5000`)
    - External database configured in environment variables (POSTGRES_HOST, POSTGRES_PORT, etc.)
    - Frontend must be built (located in `frontend/dist/`)
  - **Command**: `pytest tests/browser/ -v`
  - **For cloud agents**: Flask server should start automatically via terminals in `.cursor/environment.json`, or agents should start it manually before running tests

- **Test Structure**:
  - Use fixtures from `tests/browser/conftest.py`:
    - `page`: Basic Playwright page fixture (waits for Flask server)
    - `logged_in_page`: Page with authenticated session
    - `app_url`: Flask app URL from environment
  - Example test structure:
    ```python
    def test_my_feature(logged_in_page: Page, app_url: str) -> None:
        """Test my feature."""
        page = logged_in_page
        page.goto(f"{app_url}/my-feature")
        # Interact with elements and assert expected behavior
    ```

- **Taking Screenshots**:
  - Use `page.screenshot()` to capture screenshots during tests
  - Save to `tests/browser/screenshots/` directory with descriptive names
  - Example:
    ```python
    page.screenshot(path="tests/browser/screenshots/dashboard_after_login.png")
    ```
  - **Screenshot naming**: Use descriptive names like `feature_name_state.png` (e.g., `campaigns_page_loaded.png`, `job_details_with_notes.png`)

- **For Cloud Agents**:
  - Install Playwright: `pip install playwright && python -m playwright install chromium`
  - Build frontend: `cd frontend && npm ci && npm run build && cd ..`
  - Start Flask server: `cd campaign_ui && python app.py &` (in background)
  - Run tests: `pytest tests/browser/ -v`
  - See `tests/browser/README.md` for detailed setup instructions

- **Attaching Screenshots to Linear Issues**:
  - **When testing in QA phase**:
    1. Take screenshots using `page.screenshot()` during browser tests
    2. Save screenshots to `tests/browser/screenshots/` directory
    3. Reference screenshots in Linear issue comments using markdown:
       ```markdown
       Browser test screenshots:
       - Dashboard: `tests/browser/screenshots/dashboard_after_login.png`
       - Campaigns page: `tests/browser/screenshots/campaigns_page_loaded.png`
       ```
    4. If screenshots need to be accessible in Linear UI:
       - Option A: Commit screenshots to repository and reference in comments
       - Option B: Upload screenshots to a public location and include image links in markdown
       - Option C: Describe screenshots in comments if file attachment is not directly supported
  - **Screenshot best practices**:
    - Take screenshots at key verification points (after login, after form submission, after state changes)
    - Include screenshots in Linear comments when reporting test results
    - Name screenshots descriptively to indicate what they verify

---

## Version Control Strategy

### GitHub Operations via MCP

**IMPORTANT**: When performing GitHub-specific operations, **always use MCP (Model Context Protocol) GitHub tools** instead of direct git commands or manual GitHub UI interactions.

- **Use MCP GitHub tools for**:
  - Creating, updating, and managing issues
  - Creating, updating, reviewing, and merging pull requests
  - Creating and managing branches
  - Searching code, repositories, issues, and pull requests
  - Managing releases and tags
  - Getting repository information, file contents, and commit details
  - Any GitHub API operations

- **Use standard git commands only for**:
  - Local git operations (commit, add, status, log, etc.)
  - Local branch operations
  - Local file operations

- **When to use MCP GitHub tools**:
  - Creating pull requests: Use `mcp_github_create_pull_request` instead of pushing and creating PR manually
  - Reviewing PRs: Use `mcp_github_pull_request_read` and review tools instead of manual review
  - Managing issues: Use `mcp_github_create_issue`, `mcp_github_update_issue`, etc.
  - Searching codebase: Use `mcp_github_search_code` for cross-repository searches
  - Getting file contents from GitHub: Use `mcp_github_get_file_contents` instead of cloning

- **Best practices**:
  - Always check repository owner and name before making MCP calls
  - Use `mcp_github_get_me` to get authenticated user information when needed
  - For PR reviews, create a pending review first, add comments, then submit
  - Verify branch names and PR numbers before operations
  - Handle errors gracefully and provide clear feedback

### DigitalOcean Operations via MCP

**IMPORTANT**: When performing DigitalOcean-specific operations for environment control and deployment, **always use MCP (Model Context Protocol) DigitalOcean tools** instead of manual console interactions or SSH commands where possible.

- **Use MCP DigitalOcean tools for**:
  - Listing and managing droplets (check status, get information)
  - Managing database clusters (list, get status, check health)
  - Monitoring actions and deployment status
  - Verifying infrastructure health before deployments
  - Environment verification during QA and deployment phases

- **Use deployment scripts for**:
  - Actual code deployment (`deploy-staging.sh`, `deploy-production.sh`)
  - SSH-based operations on droplets
  - Running Docker Compose commands

- **When to use MCP DigitalOcean tools**:
  - Before deployment: Check droplet status and database cluster health
  - During QA: Verify environment is ready for testing
  - After deployment: Confirm services are running correctly
  - Troubleshooting: Get droplet/database information for debugging
  - Production deployment: Verify production environment (slot 10) status

- **Best practices**:
  - Use `droplet-list` and `droplet-get` to verify droplet status before deployments
  - Use `db-cluster-list` and `db-cluster-get` to check database cluster health
  - Use `action-list` and `action-get` to monitor deployment actions
  - Combine MCP tools with deployment scripts for complete deployment workflow
  - Always verify infrastructure status before and after deployments

### Branch Naming

**For Linear-tracked work**: See [MANDATORY: Linear Task Workflow](#mandatory-linear-task-workflow) section above. Use `linear-{issue-id}-{short-description}` format.

**For non-Linear work** (experiments, personal branches):
- Format: `[type]/[short-description]`
- Types:
  - `feature/` – New features or major components (e.g., `feature/add-ranker-service`).
  - `bugfix/` – Bug fixes (e.g., `bugfix/fix-dbt-key-violation`).
  - `chore/` – Maintenance, tooling, or config (e.g., `chore/update-ruff-config`).
  - `docs/` – Documentation changes (e.g., `docs/update-job-postings-prd`).
  - `experiment/` – Throwaway experiments or spikes (should not merge directly).

### Commit Messages

- Use a simple, consistent structure:

```text
[TYPE]: Brief description (max 50 chars)

Optional longer description wrapped at ~72 chars.

- Bullet points for key changes
- Reference tickets/issues where relevant (e.g., #123)
```

- Example:

```text
feature: Add initial jobs extractor service

- Implement JSearch API client with basic params
- Write responses into raw.jsearch_job_postings
- Add unit tests for query construction
```


### Commit Hygiene

- Keep commits **atomic**:
  - One logical change per commit (e.g., do not mix dbt refactors and Airflow DAG restructuring).
- Ensure:
  - Python tests pass (or are updated) for affected areas.
  - dbt models at least compile for schema-altering changes.
  - Linting (ruff) is clean for changed files.

### CI Status Checking After Push

**MANDATORY REQUIREMENT**: After EVERY push to the remote repository (`git push`), the agent MUST automatically check CI status and report results to the user. This is a required step that must be performed without user prompting.

**Enforcement**: This check MUST happen immediately after any successful `git push` command completes. Do not wait for user instruction - perform the check automatically.

**CRITICAL**: The agent MUST continue polling CI status until the workflow run reaches a terminal state ("completed", "failure", "cancelled", or "skipped"). Do NOT stop polling after a fixed timeout. Keep checking until completion.

#### Workflow

1. **Wait for CI initiation**: After push completes, wait 10-15 seconds for GitHub Actions to start the workflow run, then begin polling.

2. **Poll for CI completion** (MANDATORY - continue until completion): 
   - **Poll continuously** every 15-30 seconds
   - **DO NOT stop polling** until the workflow status is a terminal state:
     - `completed` (success or failure)
     - `failure`
     - `cancelled`
     - `skipped`
   - Check status using `.github/scripts/query_ci_errors.py`:
     ```bash
     python .github/scripts/query_ci_errors.py \
       --repo <owner>/<repo> \
       --token $GITHUB_TOKEN \
       --latest \
       --branch <branch_name> \
       --jobs \
       --output-json
     ```
   - **Continue polling indefinitely** until terminal state is reached
   - If workflow is still `in_progress` or `queued`, wait and check again
   - Only stop when status is `completed`, `failure`, `cancelled`, or `skipped`

3. **Report CI status to user**:
   
   **If CI passed**:
   - Inform user: "✅ CI checks passed successfully for branch <branch_name>"
   - Show summary of jobs that ran (lint-and-format, test, dbt-test)
   
   **If CI failed**:
   - Extract detailed error information using `.github/scripts/report_ci_errors.py`:
     ```bash
     python .github/scripts/report_ci_errors.py \
       --workflow-run-id <run_id> \
       --repo <owner>/<repo> \
       --token $GITHUB_TOKEN \
       --output-json
     ```
   - Present a clear, structured error report:
     - **Summary**: List failed jobs and total error count
     - **For each failed job**, show:
       - **Linting errors**: File path, line:column, error code, and message
       - **Test failures**: Test name and error details (truncate long messages)
       - **dbt test failures**: Test name, location, and error details
     - Include link to the workflow run in GitHub Actions UI
   - **Offer to fix**: Ask user if they want help fixing the errors

4. **Error handling**:
   - If GitHub API access fails: "❌ Unable to check CI status. Ensure GITHUB_TOKEN is set with repo and actions:read permissions."
   - If workflow is stuck in `in_progress` or `queued` for an unusually long time (>30 minutes), inform user: "⚠️ CI workflow is taking longer than expected. Current status: <status>. Continue monitoring or check GitHub Actions UI manually: <url>"
   - **Note**: Do not give up on polling - continue checking until a terminal state is reached

#### Implementation Notes

- **Continuous Polling**: The agent MUST implement a polling loop that continues until CI reaches a terminal state. Do not exit the polling loop early.
- Use `github.repository` and current branch name from git context
- Store workflow run ID from initial status check for error extraction
- Parse JSON output from scripts programmatically
- Format error output in a readable, structured manner for the user
- If user requests fixes, analyze errors and suggest code changes
- **Polling Strategy**:
  - Start polling 10-15 seconds after push completes
  - Poll every 15-30 seconds
  - Check workflow status on each poll
  - Continue until status is `completed`, `failure`, `cancelled`, or `skipped`
  - Log polling progress to user (e.g., "Waiting for CI to complete... (status: in_progress)")
- **Token Access**: 
  - Preferred: Use `GITHUB_TOKEN` environment variable if available
  - Alternative: Use MCP GitHub tools if available (mcp_github_list_commits, etc.)
  - Fallback: Prompt user if token is not available
- **Dependencies**: Scripts require `PyGithub` and `requests` packages. Install with `pip install PyGithub requests` if running locally.

---

## Code Review Checklist

Before considering code "done", check:

- **Correctness & Requirements**
  - [ ] Implementation aligns with `Project Documentation/job-postings-prd.md`.
  - [ ] Data structures (schemas, tables, columns) follow naming conventions.
  - [ ] No behavior conflicts with the Medallion architecture or service boundaries.

- **Python Code Quality**
  - [ ] Functions and classes have appropriate type hints.
  - [ ] Public functions/methods have meaningful docstrings.
  - [ ] No unused imports or variables.
  - [ ] Logging is used instead of `print`, with appropriate log levels.
  - [ ] Code follows bets practicel like DRY code and Separation Of Concerns.

- **dbt Models & SQL**
  - [ ] Model names and schema locations are correct (raw/staging/marts).
  - [ ] CTEs are named clearly; complex logic is broken into manageable pieces.
  - [ ] dbt schema files updated with relevant tests for new/changed models.

- **Testing**
  - [ ] New logic is covered by unit tests where feasible.
  - [ ] Relevant integration or pipeline tests updated or added.
  - [ ] `dbt test` succeeds for impacted models (where applicable).

- **Operational Concerns**
  - [ ] Airflow DAG changes preserve task order and idempotency where needed.
  - [ ] External API usage (JSearch, Glassdoor) respects rate limits and handles errors.
  - [ ] Secrets are not hard-coded (use env vars).

- **Documentation**
  - [ ] Documentation updated where needed (PRD sections, design notes, or inline comments).

---

## Reference Documents

When making design or implementation decisions, consult:

- **Product Requirements**:
  - `Project Documentation/job-postings-prd.md`
- **API Usage and Payloads**:
  - `Project Documentation/jsearch.md`
  - `Project Documentation/glassdoor_companies.md` (if present)
- **Data Modeling & Naming**:
  - `Project Documentation/naming_conventions.md`
  - Any data modeling diagrams in `Project Documentation/` (e.g., data modeling and architecture diagrams).
- **Implementation Planning**:
  - `Project Documentation/implementation-todo.md` (if present) or similar planning docs.

If new patterns, tables, or services are introduced, they should be:

1. Consistent with these rules.
2. Reflected in the PRD and/or supporting documentation.
3. Explained in code comments/docstrings where non-obvious.

---

## Linear Workflow

**IMPORTANT**: Before starting any Linear task, read the [MANDATORY: Linear Task Workflow](#mandatory-linear-task-workflow) section at the top of this file.

Use Linear to track work items related to this repository. Follow the workflow steps below for each issue lifecycle phase.

### Workspace Context

- **Team**: `Job_search_assistant`
- **Project**: `Job Search Assistant - Project`
- **Labels**: `Bug`, `Feature`, `Improvement`, `Investigation`
- **Statuses**:
  - Backlog: `New`, `Soon`, `Later`
  - Unstarted: `Todo`, `Preparation`, `Fixes needed`
  - Started: `In Progress`, `Code review`, `QA`
  - Completed: `Ready to Deploy`, `Deployed & Pending`, `Done`
  - Other: `Blocked`

### Workflow Steps

#### 1. Creating New Issues

When asked to create a new issue:

- **Search first**: Use `list_issues` with a query to check for duplicates before creating.
- **Create with `New` status**: All newly created issues must start with status `New`.
- **Set metadata**:
  - Always set `project` to `Job Search Assistant - Project`.
  - Assign the most specific label available (`Bug`, `Feature`, `Improvement`, or `Investigation`).
  - Add a clear, factual description of what needs to be done and why.

#### 2. Starting Work on an Issue

**Note**: For agent workflows, see `project_documentation/agent-workflow.md`. Agents automatically handle worktree creation and status updates.

When manually starting work on an issue:

- **Find the issue**: Use the provided issue ID or search by title/query.
- **Create branch**: Create a new branch using format `linear-<issue-id>-<short-description>` (e.g., `linear-abc123-add-user-authentication`).
  - **CRITICAL**: See [MANDATORY: Linear Task Workflow](#mandatory-linear-task-workflow) section for branch naming rules.
  - Use separate branches for each issue (even if working on multiple issues).
- **Update status**: Change issue status to `In Progress` (for Development Agent) or appropriate status for other phases.
- **Begin implementation**: Follow the [User Request Handling](#user-request-handling) workflow.

#### 3. Completing Work and Moving to Code Review

When implementation is complete:

- **Leave a comment** on the Linear issue with:
  - **Summary of changes**: Brief description of what was implemented.
  - **PR link**: Link to the pull request (if created) or branch name.
  - **Testing notes**: What was tested locally (unit tests, integration tests, manual verification).
- **Update status**: Move issue to `Code review`.

#### 4. Conducting Code Review

When asked to do code review:

- **Review according to rules**: Follow the [Code Review Checklist](#code-review-checklist) section.
- **Check for**:
  - Correctness and alignment with requirements.
  - Code quality (type hints, docstrings, naming conventions).
  - Test coverage.
  - Operational concerns (error handling, logging, secrets).
- **If issues found**:
  - Add review comments to the PR (using MCP GitHub tools).
  - Move Linear issue to `Fixes needed` status.
  - Leave a comment on the Linear issue explaining what needs to be fixed.
- **If review passes**:
  - Approve the PR (if using MCP GitHub tools).
  - Move Linear issue to `QA` status.
  - Leave a comment on the Linear issue confirming review completion.

#### 5. Quality Assurance (QA)

When issue is in `QA` status:

- **Run relevant tests**: Execute only tests related to the changes (unit tests, integration tests).
- **Run linting**: Execute `ruff` to ensure code quality.
- **Browser verification**: Test UI functionality that was affected by the changes.
  - **If UI changes were made**: Run browser tests using Playwright:
    - Ensure Flask server is running (start with `cd campaign_ui && python app.py &` if needed)
    - Ensure frontend is built (`cd frontend && npm run build`)
    - Run browser tests: `pytest tests/browser/ -v`
    - Take screenshots during tests at key verification points using `page.screenshot()`
    - Save screenshots to `tests/browser/screenshots/` with descriptive names
  - **Manual browser testing** (if needed):
    - Navigate to relevant pages/components.
    - Verify expected behavior.
    - Document any issues found.
- **Document results**: Leave a comment on the Linear issue with:
  - Test results (pass/fail summary).
  - Linting results.
  - Browser verification results:
    - Browser test results (if applicable)
    - Manual testing notes (if applicable)
  - **Screenshot attachment**: Include screenshots in Linear comments:
    - Reference screenshot file paths in markdown format
    - Describe what each screenshot verifies
    - Example comment format:
      ```markdown
      Browser Test Results:
      ✅ All browser tests passed
      
      Screenshots:
      - Dashboard after login: `tests/browser/screenshots/dashboard_after_login.png`
      - Campaigns page: `tests/browser/screenshots/campaigns_page_loaded.png`
      
      Verification:
      - Login flow works correctly
      - Dashboard displays expected data
      - Navigation between pages works
      ```
- **If QA passes**: Move issue to `Ready to Deploy`.
- **If QA fails**: Move issue to `Fixes needed` and document issues in comment (include screenshots of any failures if applicable).

#### 6. Deployment

When asked to deploy an issue in `Ready to Deploy` status:

- **Push to remote**: Push the branch to the remote repository.
- **Create/update PR**: Use MCP GitHub tools to create or update the pull request.
- **Monitor CI**: Follow the [CI Status Checking After Push](#ci-status-checking-after-push) workflow.
  - Poll CI status until completion (terminal state).
  - If CI fails:
    - Extract and analyze errors.
    - Fix CI-related issues (linting errors, test failures, dbt test failures).
    - Push fixes and continue monitoring until CI passes.
    - Repeat until CI is successful.
- **Merge PR**: Once CI passes successfully:
  - Merge the pull request using MCP GitHub tools.
  - Move Linear issue to `Done` status.
  - Leave a comment on the Linear issue confirming successful deployment.

#### 7. Handling Blocked Issues

If unable to proceed with work due to external reasons:

- **Move to `Blocked`**: Update issue status to `Blocked`.
- **Leave detailed comment** explaining:
  - **What's blocking**: Clear description of the blocker (missing information, dependency, external issue, etc.).
  - **What's needed to unblock**: Specific actions or information required to proceed.
  - **Context**: Any relevant details that would help resolve the blocker.

### Branch Naming for Linear Issues

**See [MANDATORY: Linear Task Workflow](#mandatory-linear-task-workflow) section for branch naming rules.**

When creating branches for Linear issues, use format: `linear-<issue-id>-<short-description>`

- Example: `linear-abc123-add-user-authentication`
- The `linear-` prefix distinguishes Linear-tracked work.
- Include the Linear issue ID for traceability.
- Keep description short and descriptive.
- **For agents**: Branch is created automatically when creating the worktree (see `agent-workflow.md`).

### Multiple Issues

- **Always use separate branches**: Each Linear issue must have its own branch.
- **Do not combine work**: Even if issues are related, maintain separate branches and PRs.
- **Track independently**: Update each issue's status independently as work progresses.

### Metadata Consistency

- **Always set project**: Every issue must have `project` set to `Job Search Assistant - Project`.
- **Use appropriate labels**: Choose the most specific label (`Bug`, `Feature`, `Improvement`, `Investigation`).
- **Keep descriptions factual**: Write clear, concise descriptions without AI references.

### When to Update Linear

- **Always update** when:
  - Creating new issues (status: `New`).
  - Starting work (status: `In Progress`).
  - Completing work (status: `Code review` with comment).
  - Completing code review (status: `QA` or `Fixes needed` with comment).
  - Completing QA (status: `Ready to Deploy` or `Fixes needed` with comment).
  - Deploying (status: `Done` with comment).
  - Blocked (status: `Blocked` with comment).
- **Do not update** if the user explicitly asks not to track work in Linear.

---

## Agent Workflow

**For complete agent workflow documentation, see `project_documentation/agent-workflow.md`.**

This section provides a quick reference for status-based agent triggers. Detailed workflow steps, worktree management, staging slot management, and comment templates are in `agent-workflow.md`.

### Agent Status-Based Triggers

Agents automatically pick up tasks based on Linear issue status. No tags needed.

| Status | Agent | Responsibilities |
|--------|-------|------------------|
| `Todo` | Development Agent | Create worktree, implement, test, push, create PR |
| `Fixes needed` | Development Agent | Address feedback, push fixes |
| `Code review` | Review Agent | Review code, approve or request changes |
| `QA` | QA Agent | Claim staging slot, deploy, verify, attach evidence |
| `Ready to Deploy` | Deploy Agent | Merge PR, monitor CI, release staging, cleanup |
| Any (with CI failure) | CI Fix Agent | Analyze failures, push fixes, re-trigger CI |

### Key Principles

- **Isolation**: One task = One worktree = One branch = One staging slot (if relevant)
- **Status-driven**: Status changes automatically trigger next agent phase
- **MCP tools**: Use Linear MCP for status/comments, GitHub MCP for PRs, DigitalOcean MCP for environment control and deployment
- **Worktrees**: Each issue gets its own worktree in `.worktrees/linear-{issue-id}-{description}/`

**For detailed workflow steps, worktree management, staging slot protocols, and comment templates, see `project_documentation/agent-workflow.md`.**
